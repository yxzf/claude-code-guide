# Model Context Protocol (MCP) å®Œæ•´æŒ‡å—

> **ä½œè€…**: Claude Code Assistant  
> **ç‰ˆæœ¬**: 2.0  
> **æœ€åæ›´æ–°**: 2025å¹´01æœˆ  
> **é€‚ç”¨èŒƒå›´**: AIåº”ç”¨å¼€å‘è€…ã€ç³»ç»Ÿæ¶æ„å¸ˆã€äº§å“ç»ç†

---

## ğŸ“– æ–‡æ¡£æ¦‚è§ˆ

æœ¬æŒ‡å—ä¸ºä½ æä¾› Model Context Protocol (MCP) çš„å…¨é¢ç†è§£ï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§å®ç°ï¼Œæ¶µç›–ç†è®ºä¸å®è·µã€‚æ— è®ºä½ æ˜¯åˆå­¦è€…è¿˜æ˜¯ç»éªŒä¸°å¯Œçš„å¼€å‘è€…ï¼Œéƒ½èƒ½åœ¨è¿™é‡Œæ‰¾åˆ°æ‰€éœ€çš„çŸ¥è¯†ã€‚

### ğŸ¯ å­¦ä¹ è·¯å¾„
- **æ–°æ‰‹**: å…ˆè¯»ç¬¬1-3ç« äº†è§£åŸºç¡€æ¦‚å¿µ â†’ ç¬¬5ç« å¿«é€Ÿä¸Šæ‰‹ â†’ ç¬¬6ç« å®è·µæ¡ˆä¾‹
- **å¼€å‘è€…**: é‡ç‚¹å…³æ³¨ç¬¬4ç« æŠ€æœ¯åŸç† â†’ ç¬¬5ç« å¼€å‘æŒ‡å— â†’ ç¬¬7ç« æœ€ä½³å®è·µ
- **æ¶æ„å¸ˆ**: æ·±å…¥ç¬¬2ç« æ¶æ„è®¾è®¡ â†’ ç¬¬4ç« å®ç°åŸç† â†’ ç¬¬8ç« å¯¹æ¯”åˆ†æ

---

## ğŸ“‹ ç›®å½•ç»“æ„

### ğŸ—ï¸ ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç†è®º
1. [MCP æ ¸å¿ƒæ¦‚å¿µ](#1-mcp-æ ¸å¿ƒæ¦‚å¿µ)
2. [æ¶æ„è®¾è®¡åŸç†](#2-æ¶æ„è®¾è®¡åŸç†)  
3. [åè®®ä¸æ ‡å‡†](#3-åè®®ä¸æ ‡å‡†)

### ğŸ’» ç¬¬äºŒéƒ¨åˆ†ï¼šæŠ€æœ¯å®ç°
4. [å·¥ä½œåŸç†æ·±åº¦è§£æ](#4-å·¥ä½œåŸç†æ·±åº¦è§£æ)
5. [å¼€å‘å®æˆ˜æŒ‡å—](#5-å¼€å‘å®æˆ˜æŒ‡å—)
6. [å®é™…åº”ç”¨æ¡ˆä¾‹](#6-å®é™…åº”ç”¨æ¡ˆä¾‹)

### ğŸš€ ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¿›é˜¶ä¸å®è·µ
7. [æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ](#7-æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ)
8. [ç”Ÿæ€ç³»ç»Ÿä¸å·¥å…·](#8-ç”Ÿæ€ç³»ç»Ÿä¸å·¥å…·)
9. [å¯¹æ¯”åˆ†æä¸é€‰å‹](#9-å¯¹æ¯”åˆ†æä¸é€‰å‹)
10. [å‘å±•è¶‹åŠ¿ä¸å±•æœ›](#10-å‘å±•è¶‹åŠ¿ä¸å±•æœ›)

---

## 1. MCP æ ¸å¿ƒæ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ MCPï¼Ÿ

**Model Context Protocol (MCP)** æ˜¯ç”± Anthropic äº 2024å¹´11æœˆ25æ—¥ å‘å¸ƒçš„å¼€æ”¾åè®®ï¼Œä¸“é—¨ç”¨äºæ ‡å‡†åŒ– AI åº”ç”¨ç¨‹åºä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„äº¤äº’æ–¹å¼ã€‚

### MCP æ ¸å¿ƒæ¶æ„

MCP é‡‡ç”¨å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„è®¾è®¡ï¼ŒAIåº”ç”¨é€šè¿‡MCPå®¢æˆ·ç«¯ä¸å¤šä¸ªMCPæœåŠ¡å™¨å»ºç«‹ä¸€å¯¹ä¸€è¿æ¥ï¼š

![MCPæ ¸å¿ƒæ¶æ„å›¾](images/mcp_official_architecture.png)

**æ¶æ„è¯´æ˜**ï¼š
- **MCP Host (AIåº”ç”¨)**ï¼šå¦‚Claude Desktopã€VS Codeç­‰ï¼Œè´Ÿè´£åè°ƒç®¡ç†å¤šä¸ªMCPå®¢æˆ·ç«¯
- **MCP Client**ï¼šæ¯ä¸ªå®¢æˆ·ç«¯ç»´æŠ¤ä¸ä¸€ä¸ªMCPæœåŠ¡å™¨çš„ä¸“ç”¨è¿æ¥
- **MCP Server**ï¼šæä¾›å…·ä½“åŠŸèƒ½çš„æœåŠ¡ç«¯ï¼Œå¦‚Sentryã€æ–‡ä»¶ç³»ç»Ÿã€æ•°æ®åº“ç­‰

**è¿æ¥æ¨¡å¼**ï¼šé‡‡ç”¨ä¸€å¯¹ä¸€è¿æ¥æ¨¡å¼ï¼Œç¡®ä¿æ¯ä¸ªMCPå®¢æˆ·ç«¯ä¸å¯¹åº”çš„MCPæœåŠ¡å™¨å»ºç«‹ç‹¬ç«‹çš„é€šä¿¡é€šé“ã€‚

#### ğŸ’¡ æ ¸å¿ƒç±»æ¯”ï¼šAI ä¸–ç•Œçš„ USB-C
å°±åƒ USB-C ä¸ºå„ç§è®¾å¤‡æä¾›äº†ç»Ÿä¸€çš„è¿æ¥æ ‡å‡†ï¼ŒMCP ä¸º AI æ¨¡å‹ä¸å¤–éƒ¨èµ„æºæä¾›äº†ç»Ÿä¸€çš„äº¤äº’åè®®ã€‚

```
ä¼ ç»Ÿæ–¹å¼ (æ··ä¹±):
AIåº”ç”¨ â”€â”€â”¬â”€â†’ OpenAI Functions â”€â”€â†’ å·¥å…·A
         â”œâ”€â†’ Google Extensions â”€â”€â†’ å·¥å…·B  
         â””â”€â†’ è‡ªå®šä¹‰API â”€â”€â†’ å·¥å…·C

MCPæ–¹å¼ (ç»Ÿä¸€):
AIåº”ç”¨ â”€â”€â†’ MCPåè®® â”€â”€â”¬â”€â†’ MCPæœåŠ¡å™¨A
                      â”œâ”€â†’ MCPæœåŠ¡å™¨B
                      â””â”€â†’ MCPæœåŠ¡å™¨C
```

### 1.2 è§£å†³çš„æ ¸å¿ƒé—®é¢˜

#### ğŸ¤” ä¼ ç»Ÿ AI å·¥å…·é›†æˆçš„å›°å¢ƒ

| é—®é¢˜ | å…·ä½“è¡¨ç° | å½±å“ |
|------|----------|------|
| **å¹³å°é”å®š** | æ¯ä¸ª LLM æä¾›å•†éƒ½æœ‰ä¸åŒçš„å‡½æ•°è°ƒç”¨ API | åˆ‡æ¢æ¨¡å‹æˆæœ¬é«˜ï¼Œå‚å•†ä¾èµ–ä¸¥é‡ |
| **é‡å¤å¼€å‘** | ä¸ºæ¯ä¸ªå¹³å°é‡å†™ç›¸åŒçš„å·¥å…·é›†æˆ | å¼€å‘æ•ˆç‡ä½ï¼Œç»´æŠ¤æˆæœ¬é«˜ |
| **å®‰å…¨éšæ‚£** | æ•æ„Ÿæ•°æ®éœ€è¦ä¸Šä¼ åˆ°äº‘ç«¯å¤„ç† | æ•°æ®æ³„éœ²é£é™©ï¼Œåˆè§„éš¾é¢˜ |
| **ç¼ºä¹æ ‡å‡†** | æ²¡æœ‰ç»Ÿä¸€çš„å·¥å…·æè¿°å’Œè°ƒç”¨è§„èŒƒ | ç”Ÿæ€ç³»ç»Ÿåˆ†æ•£ï¼Œäº’æ“ä½œæ€§å·® |

#### âœ… MCP çš„è§£å†³æ–¹æ¡ˆ

**ç»Ÿä¸€åè®®å±‚**: ä¸€æ¬¡å¼€å‘ï¼Œå¤„å¤„è¿è¡Œ
- **å¼€å‘ä¸€å¥—å·¥å…·** â†’ æ‰€æœ‰æ”¯æŒ MCP çš„ AI åº”ç”¨éƒ½èƒ½ä½¿ç”¨
- **æ•°æ®æœ¬åœ°åŒ–** â†’ æ•æ„Ÿä¿¡æ¯æ— éœ€ä¸Šä¼ ï¼Œå®‰å…¨å¯æ§
- **æ ‡å‡†åŒ–æ¥å£** â†’ ç»Ÿä¸€çš„å·¥å…·æè¿°ã€è°ƒç”¨å’Œå“åº”æ ¼å¼
- **å¼€æºç”Ÿæ€** â†’ ç¤¾åŒºé©±åŠ¨ï¼ŒæŒç»­æ¼”è¿›

### 1.3 æ ¸å¿ƒä»·å€¼ä¸»å¼ 

#### ğŸ¯ å¯¹å¼€å‘è€…
- **å‡å°‘ 80% çš„é‡å¤å·¥ä½œ**: ä¸€æ¬¡å¼€å‘ï¼Œå¤šå¹³å°å¤ç”¨
- **é™ä½å­¦ä¹ æ›²çº¿**: ç»Ÿä¸€çš„å¼€å‘æ¨¡å¼å’Œ API
- **ä¸°å¯Œçš„ç”Ÿæ€**: 100+ ç°æˆçš„ MCP æœåŠ¡å™¨å¯ç›´æ¥ä½¿ç”¨

#### ğŸ¢ å¯¹ä¼ä¸š
- **æ•°æ®å®‰å…¨**: æ•æ„Ÿæ•°æ®ç•™åœ¨æœ¬åœ°ï¼Œç²¾ç¡®æ§åˆ¶è®¿é—®æƒé™  
- **é™ä½æˆæœ¬**: é¿å…å‚å•†é”å®šï¼Œçµæ´»é€‰æ‹© AI æ¨¡å‹
- **å¿«é€Ÿé›†æˆ**: æ ‡å‡†åŒ–æ¥å£ï¼ŒåŠ é€Ÿ AI é¡¹ç›®è½åœ°

#### ğŸ‘¥ å¯¹ç”¨æˆ·
- **æ›´æ™ºèƒ½çš„ AI**: èƒ½è®¿é—®å®æ—¶æ•°æ®å’Œä¸“ä¸šå·¥å…·
- **æ— ç¼ä½“éªŒ**: åœ¨ä¸åŒåº”ç”¨é—´ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
- **éšç§ä¿æŠ¤**: æ•°æ®å¤„ç†é€æ˜å¯æ§

---

## 2. æ¶æ„è®¾è®¡åŸç†

### 2.1 æ•´ä½“æ¶æ„æ¦‚è§ˆ

MCP é‡‡ç”¨ç»å…¸çš„**å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„**ï¼Œé€šè¿‡æ ‡å‡†åŒ–çš„åè®®å®ç° AI åº”ç”¨ä¸å¤–éƒ¨èµ„æºçš„è¿æ¥ã€‚

```mermaid
graph TB
    subgraph "Host Layer (AIåº”ç”¨å±‚)"
        H1[Claude Desktop]
        H2[VS Code + Cursor]  
        H3[è‡ªå®šä¹‰AIåº”ç”¨]
    end
    
    subgraph "Client Layer (å®¢æˆ·ç«¯å±‚)"
        C1[MCP Client 1]
        C2[MCP Client 2]
        C3[MCP Client 3]
    end
    
    subgraph "Server Layer (æœåŠ¡å™¨å±‚)"
        S1[æ–‡ä»¶ç³»ç»ŸæœåŠ¡å™¨]
        S2[æ•°æ®åº“æœåŠ¡å™¨]
        S3[APIé›†æˆæœåŠ¡å™¨]
    end
    
    subgraph "Data Layer (æ•°æ®å±‚)"
        D1[æœ¬åœ°æ–‡ä»¶]
        D2[äº‘ç«¯æ•°æ®åº“]
        D3[ç¬¬ä¸‰æ–¹API]
    end
    
    H1 -.->|ç®¡ç†| C1
    H2 -.->|ç®¡ç†| C2
    H3 -.->|ç®¡ç†| C3
    
    C1 <-.->|JSON-RPC 2.0| S1
    C2 <-.->|JSON-RPC 2.0| S2
    C3 <-.->|JSON-RPC 2.0| S3
    
    S1 -.->|è®¿é—®| D1
    S2 -.->|è®¿é—®| D2
    S3 -.->|è®¿é—®| D3
```

### MCP ä»·å€¼å¯¹æ¯”

**ä¼ ç»Ÿæ–¹å¼ vs MCPæ–¹å¼**ï¼š

| ä¼ ç»Ÿæ–¹å¼ï¼ˆå¤æ‚ï¼‰ | MCPæ–¹å¼ï¼ˆç®€æ´ï¼‰ |
|-----------------|----------------|
| æ¯ä¸ªAIåº”ç”¨éœ€è¦å•ç‹¬é›†æˆå„ç§æœåŠ¡ | ç»Ÿä¸€çš„MCPåè®®å±‚ |
| é‡å¤å¼€å‘é›†æˆä»£ç  | ä¸€æ¬¡å¼€å‘ï¼Œå¤„å¤„ä½¿ç”¨ |
| ç»´æŠ¤æˆæœ¬é«˜ | æ ‡å‡†åŒ–ç»´æŠ¤ |
| åˆ‡æ¢åº”ç”¨æˆæœ¬é«˜ | æ— ç¼è¿ç§» |

**æ¶æ„å¯¹æ¯”**ï¼š
```
âŒ æ²¡æœ‰MCPï¼šAIåº”ç”¨ â†â†’ å„ç§æœåŠ¡ï¼ˆæ··ä¹±çš„å¤šå¯¹å¤šè¿æ¥ï¼‰
âœ… æœ‰äº†MCPï¼šAIåº”ç”¨ â†â†’ MCPåè®® â†â†’ å„ç§æœåŠ¡ï¼ˆæ¸…æ™°çš„åˆ†å±‚ï¼‰
```

### 2.2 æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### ğŸ–¥ï¸ MCP Host (ä¸»æœº)
**èŒè´£**: AI åº”ç”¨çš„åè°ƒä¸­å¿ƒ
- æ¥æ”¶ç”¨æˆ·è¾“å…¥å¹¶ä¸ AI æ¨¡å‹äº¤äº’
- ç®¡ç†å¤šä¸ª MCP å®¢æˆ·ç«¯çš„ç”Ÿå‘½å‘¨æœŸ
- å†³ç­–ä½•æ—¶è°ƒç”¨å“ªäº›å·¥å…·
- æ•´åˆç»“æœå¹¶å‘ˆç°ç»™ç”¨æˆ·

**å…¸å‹å®ç°**:
- **Claude Desktop**: Anthropic å®˜æ–¹å®¢æˆ·ç«¯
- **VS Code**: é€šè¿‡æ’ä»¶æ”¯æŒ
- **Cursor**: ä»£ç ç¼–è¾‘å™¨é›†æˆ
- **è‡ªå®šä¹‰åº”ç”¨**: ä½¿ç”¨ MCP SDK å¼€å‘

#### ğŸ”Œ MCP Client (å®¢æˆ·ç«¯)
**èŒè´£**: åè®®é€šä¿¡çš„æ¡¥æ¢
- ä¸ç‰¹å®šçš„ MCP æœåŠ¡å™¨å»ºç«‹**ä¸€å¯¹ä¸€è¿æ¥**
- å¤„ç† JSON-RPC 2.0 åè®®é€šä¿¡
- ç®¡ç†è¿æ¥ç”Ÿå‘½å‘¨æœŸå’Œé”™è¯¯å¤„ç†
- ä¸ºä¸»æœºæä¾›ç»Ÿä¸€çš„å·¥å…·è°ƒç”¨æ¥å£

**ç‰¹ç‚¹**:
```python
# ä¸€ä¸ªä¸»æœºå¯ä»¥ç®¡ç†å¤šä¸ªå®¢æˆ·ç«¯
host = MCPHost()
client1 = host.create_client("filesystem-server")
client2 = host.create_client("database-server")  
client3 = host.create_client("api-server")
```

#### âš™ï¸ MCP Server (æœåŠ¡å™¨)
**èŒè´£**: å…·ä½“åŠŸèƒ½çš„å®ç°è€…
- å®ç°ç‰¹å®šé¢†åŸŸçš„å·¥å…·å’Œèµ„æº
- å¤„ç†æ¥è‡ªå®¢æˆ·ç«¯çš„è¯·æ±‚
- è®¿é—®å’Œæ“ä½œåº•å±‚æ•°æ®æº
- æä¾›æ ‡å‡†åŒ–çš„å“åº”æ ¼å¼

**åˆ†ç±»**:
| ç±»å‹ | è¿è¡Œä½ç½® | ä¼ è¾“æ–¹å¼ | æ€§èƒ½ | å®‰å…¨æ€§ |
|------|----------|----------|------|--------|
| **æœ¬åœ°æœåŠ¡å™¨** | åŒä¸€æœºå™¨ | STDIO | æœ€ä¼˜ | é«˜ |
| **è¿œç¨‹æœåŠ¡å™¨** | è¿œç¨‹ä¸»æœº | HTTP/SSE | ä¸€èˆ¬ | ä¸­ |

### 2.3 åè®®åˆ†å±‚è®¾è®¡

MCP é‡‡ç”¨**åŒå±‚æ¶æ„**è®¾è®¡ï¼Œåˆ†ç¦»å…³æ³¨ç‚¹ï¼š

#### ğŸ“Š æ•°æ®å±‚ (Data Layer)
**åŸºäº JSON-RPC 2.0 çš„åè®®å±‚**

```json
{
  "jsonrpc": "2.0",
  "method": "tools/call",
  "params": {
    "name": "filesystem_read",
    "arguments": {
      "path": "/Users/example/document.txt"
    }
  },
  "id": 1
}
```

**æ ¸å¿ƒåŠŸèƒ½**:
- **ç”Ÿå‘½å‘¨æœŸç®¡ç†**: åˆå§‹åŒ–ã€èƒ½åŠ›åå•†ã€ç»ˆæ­¢
- **åŸè¯­å®šä¹‰**: Toolsã€Resourcesã€Prompts
- **é€šçŸ¥æœºåˆ¶**: å®æ—¶æ›´æ–°ã€è¿›åº¦è·Ÿè¸ª

#### ğŸŒ ä¼ è¾“å±‚ (Transport Layer)
**è´Ÿè´£å®é™…çš„ç½‘ç»œé€šä¿¡**

```python
# STDIO ä¼ è¾“ (æœ¬åœ°)
transport = StdioServerTransport()
await transport.run(server)

# HTTP ä¼ è¾“ (è¿œç¨‹)  
transport = SseServerTransport("/sse")
await transport.run(server, host="0.0.0.0", port=8080)
```

**ä¼ è¾“å¯¹æ¯”**:
| ç‰¹æ€§ | STDIO | HTTP/SSE |
|------|-------|----------|
| **é€‚ç”¨åœºæ™¯** | æœ¬åœ°å·¥å…· | è¿œç¨‹æœåŠ¡ |
| **æ€§èƒ½** | æ— ç½‘ç»œå¼€é”€ | æœ‰ç½‘ç»œå»¶è¿Ÿ |
| **å®‰å…¨** | è¿›ç¨‹çº§éš”ç¦» | éœ€è¦è®¤è¯æœºåˆ¶ |
| **éƒ¨ç½²** | ç®€å• | å¤æ‚ |

---

## 3. åè®®ä¸æ ‡å‡†

### 3.1 MCP åŸè¯­ (Primitives)

MCP å®šä¹‰äº†ä¸‰ç§æ ¸å¿ƒåŸè¯­ï¼Œè¦†ç›– AI ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’çš„ä¸»è¦åœºæ™¯ï¼š

#### ğŸ”§ Tools (å·¥å…·)
**å¯æ‰§è¡Œçš„å‡½æ•°ï¼ŒAI å¯ä»¥è°ƒç”¨æ¥æ‰§è¡Œæ“ä½œ**

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("æ–‡ä»¶ç®¡ç†å·¥å…·")

@mcp.tool()
def search_files(pattern: str, directory: str = ".") -> str:
    """åœ¨æŒ‡å®šç›®å½•ä¸­æœç´¢æ–‡ä»¶
    
    Args:
        pattern: æœç´¢æ¨¡å¼ï¼Œæ”¯æŒé€šé…ç¬¦ (å¦‚ *.py, test_*)
        directory: æœç´¢ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
        
    Returns:
        str: æ‰¾åˆ°çš„æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªæ–‡ä»¶è·¯å¾„
        
    Examples:
        æœç´¢Pythonæ–‡ä»¶: search_files("*.py", "/home/project")
        æœç´¢æµ‹è¯•æ–‡ä»¶: search_files("test_*", "./tests")
    """
    import glob
    import os
    
    search_path = os.path.join(directory, pattern)
    files = glob.glob(search_path, recursive=True)
    
    if not files:
        return f"æœªåœ¨ {directory} ä¸­æ‰¾åˆ°åŒ¹é… '{pattern}' çš„æ–‡ä»¶"
    
    return "\n".join(sorted(files))
```

**ç‰¹ç‚¹**:
- âœ… **éœ€è¦ç”¨æˆ·æˆæƒ**: ç¡®ä¿å®‰å…¨æ€§
- âœ… **å¯ä»¥ä¿®æ”¹çŠ¶æ€**: èƒ½å¤Ÿæ‰§è¡Œå†™æ“ä½œ
- âœ… **æ”¯æŒå¤æ‚å‚æ•°**: ç±»å‹æ£€æŸ¥å’ŒéªŒè¯
- âœ… **è¿”å›ç»“æ„åŒ–æ•°æ®**: JSON æˆ–æ–‡æœ¬æ ¼å¼

#### ğŸ“„ Resources (èµ„æº)
**ä¸º AI æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ•°æ®æº**

```python
@mcp.resource("config://app-settings")
def get_app_settings() -> str:
    """è·å–åº”ç”¨ç¨‹åºé…ç½®ä¿¡æ¯"""
    import json
    
    config = {
        "database": {
            "host": "localhost",
            "port": 5432,
            "name": "myapp_db"
        },
        "features": {
            "authentication": True,
            "logging": True,
            "cache": False
        },
        "api_limits": {
            "requests_per_minute": 1000,
            "max_file_size": "10MB"
        }
    }
    
    return json.dumps(config, indent=2, ensure_ascii=False)

@mcp.resource("logs://recent-errors")  
def get_recent_errors() -> str:
    """è·å–æœ€è¿‘çš„é”™è¯¯æ—¥å¿—"""
    # æ¨¡æ‹Ÿè¯»å–æ—¥å¿—æ–‡ä»¶
    errors = [
        "2025-01-15 10:30:15 ERROR: Database connection timeout",
        "2025-01-15 11:45:22 ERROR: Invalid API key for user 12345", 
        "2025-01-15 14:20:33 ERROR: File upload size exceeded limit"
    ]
    
    return "\n".join(errors)
```

**ç‰¹ç‚¹**:
- ğŸ“– **åªè¯»è®¿é—®**: ä¸èƒ½ä¿®æ”¹æ•°æ®
- ğŸ·ï¸ **æ ‡å‡†åŒ–URI**: ä½¿ç”¨ç»Ÿä¸€çš„èµ„æºæ ‡è¯†ç¬¦
- ğŸ”„ **æ”¯æŒè®¢é˜…**: å¯ä»¥ç›‘å¬èµ„æºå˜åŒ–
- ğŸ“Š **ç»“æ„åŒ–æ•°æ®**: é€šå¸¸è¿”å› JSON æ ¼å¼

#### ğŸ’¬ Prompts (æç¤ºæ¨¡æ¿)
**å¯é‡ç”¨çš„äº¤äº’æ¨¡æ¿ï¼Œå¸®åŠ©æ„å»ºæ ‡å‡†åŒ–çš„æç¤º**

```python
@mcp.prompt()
def code_review_prompt(code: str, language: str, focus: str = "è´¨é‡") -> str:
    """ä»£ç å®¡æŸ¥æç¤ºæ¨¡æ¿
    
    Args:
        code: è¦å®¡æŸ¥çš„ä»£ç 
        language: ç¼–ç¨‹è¯­è¨€
        focus: å®¡æŸ¥é‡ç‚¹ (è´¨é‡/å®‰å…¨/æ€§èƒ½)
    """
    
    focus_guidelines = {
        "è´¨é‡": [
            "ä»£ç å¯è¯»æ€§å’Œç»´æŠ¤æ€§",
            "å‘½åè§„èŒƒå’Œæ³¨é‡Šè´¨é‡", 
            "ä»£ç é‡å¤å’Œå¤æ‚åº¦",
            "é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ"
        ],
        "å®‰å…¨": [
            "è¾“å…¥éªŒè¯å’Œè¿‡æ»¤",
            "æƒé™æ£€æŸ¥å’Œè®¿é—®æ§åˆ¶",
            "æ•æ„Ÿä¿¡æ¯æ³„éœ²é£é™©",
            "å¸¸è§å®‰å…¨æ¼æ´ (XSS, SQLæ³¨å…¥ç­‰)"
        ],
        "æ€§èƒ½": [
            "ç®—æ³•å¤æ‚åº¦åˆ†æ",
            "èµ„æºä½¿ç”¨ä¼˜åŒ–",
            "å¹¶å‘å’Œå¼‚æ­¥å¤„ç†",
            "ç¼“å­˜å’Œæ•°æ®ç»“æ„é€‰æ‹©"
        ]
    }
    
    guidelines = focus_guidelines.get(focus, focus_guidelines["è´¨é‡"])
    
    return f"""
è¯·å¯¹ä»¥ä¸‹ {language} ä»£ç è¿›è¡Œä¸“ä¸šå®¡æŸ¥ï¼Œé‡ç‚¹å…³æ³¨{focus}ï¼š

```{language}
{code}
```

å®¡æŸ¥æŒ‡å—ï¼š
{chr(10).join(f'â€¢ {item}' for item in guidelines)}

è¯·æä¾›ï¼š
1. **ä»£ç è´¨é‡è¯„åˆ†** (1-10åˆ†)
2. **ä¸»è¦é—®é¢˜æ¸…å•** (æŒ‰ä¼˜å…ˆçº§æ’åº)
3. **å…·ä½“æ”¹è¿›å»ºè®®** (åŒ…å«ä»£ç ç¤ºä¾‹)
4. **æœ€ä½³å®è·µå»ºè®®**

å®¡æŸ¥æ ¼å¼è¦æ±‚ï¼š
- é—®é¢˜æè¿°è¦å…·ä½“ï¼ŒæŒ‡å‡ºå…·ä½“çš„è¡Œå·
- æä¾›å¯æ“ä½œçš„æ”¹è¿›æ–¹æ¡ˆ
- å¦‚æœ‰å¿…è¦ï¼Œæä¾›é‡æ„åçš„ä»£ç ç¤ºä¾‹
"""
```

### 3.2 å®¢æˆ·ç«¯åŸè¯­

MCP è¿˜å®šä¹‰äº†å®¢æˆ·ç«¯å¯ä»¥æä¾›çš„åŸè¯­ï¼Œä½¿æœåŠ¡å™¨èƒ½å¤Ÿåå‘è°ƒç”¨å®¢æˆ·ç«¯èƒ½åŠ›ï¼š

#### ğŸ¯ Sampling (é‡‡æ ·)
```python
# æœåŠ¡å™¨å¯ä»¥è¯·æ±‚å®¢æˆ·ç«¯çš„ AI æ¨¡å‹è¿›è¡Œæ¨ç†
async def generate_summary(data: str) -> str:
    """è®© AI ç”Ÿæˆæ•°æ®æ‘˜è¦"""
    prompt = f"è¯·ä¸ºä»¥ä¸‹æ•°æ®ç”Ÿæˆç®€æ´çš„æ‘˜è¦:\n\n{data}"
    
    response = await client.sample_completion(
        prompt=prompt,
        max_tokens=200,
        temperature=0.3
    )
    
    return response
```

#### â“ Elicitation (è¯·æ±‚ç”¨æˆ·è¾“å…¥)
```python
# æœåŠ¡å™¨å¯ä»¥è¯·æ±‚ç”¨æˆ·ç¡®è®¤æˆ–è¾“å…¥é¢å¤–ä¿¡æ¯
async def confirm_deletion(file_path: str) -> bool:
    """è¯·æ±‚ç”¨æˆ·ç¡®è®¤åˆ é™¤æ“ä½œ"""
    response = await client.request_user_input(
        prompt=f"ç¡®è®¤åˆ é™¤æ–‡ä»¶ '{file_path}' å—ï¼Ÿæ­¤æ“ä½œä¸å¯æ’¤é”€ã€‚",
        input_type="confirmation"
    )
    
    return response.lower() in ['yes', 'y', 'ç¡®è®¤', 'æ˜¯']
```

#### ğŸ“ Logging (æ—¥å¿—è®°å½•)
```python
# æœåŠ¡å™¨å¯ä»¥å‘é€æ—¥å¿—åˆ°å®¢æˆ·ç«¯
async def log_operation(operation: str, result: str):
    """è®°å½•æ“ä½œæ—¥å¿—"""
    await client.log_message(
        level="info",
        message=f"æ“ä½œå®Œæˆ: {operation}",
        data={
            "operation": operation,
            "result": result,
            "timestamp": datetime.now().isoformat()
        }
    )
```

---

## 4. æ ¸å¿ƒå·¥ä½œåŸç†

### 4.1 å·¥å…·è°ƒç”¨æµç¨‹

MCP çš„æ ¸å¿ƒæ˜¯è®© AI æ¨¡å‹æ™ºèƒ½é€‰æ‹©å’Œè°ƒç”¨å·¥å…·ï¼š

```
ç”¨æˆ·è¾“å…¥ â†’ AIåˆ†æéœ€æ±‚ â†’ é€‰æ‹©å·¥å…· â†’ æ‰§è¡Œæ“ä½œ â†’ è¿”å›ç»“æœ
```

**å…³é”®æŠ€æœ¯è¦ç‚¹**ï¼š
- AI é€šè¿‡å·¥å…·æè¿°ç†è§£åŠŸèƒ½å’Œå‚æ•°
- ä½¿ç”¨ JSON-RPC 2.0 åè®®è¿›è¡Œé€šä¿¡
- æ”¯æŒå‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†

### 4.2 è¿æ¥ç”Ÿå‘½å‘¨æœŸ

MCP è¿æ¥éµå¾ªæ ‡å‡†çš„ä¸‰é˜¶æ®µæµç¨‹ï¼š

1. **åˆå§‹åŒ–é˜¶æ®µ**ï¼šå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨åå•†åè®®ç‰ˆæœ¬å’Œæ”¯æŒçš„åŠŸèƒ½
2. **å·¥ä½œé˜¶æ®µ**ï¼šå‘ç°å’Œè°ƒç”¨å·¥å…·ã€è·å–èµ„æºã€ä½¿ç”¨æç¤ºæ¨¡æ¿
3. **ç»ˆæ­¢é˜¶æ®µ**ï¼šæ¸…ç†è¿æ¥å’Œé‡Šæ”¾èµ„æº

### 4.3 å®æ—¶é€šçŸ¥æœºåˆ¶

MCP æ”¯æŒæœåŠ¡å™¨ä¸»åŠ¨æ¨é€æ›´æ–°ï¼š
- **å·¥å…·/èµ„æºå˜æ›´é€šçŸ¥**ï¼šå½“å¯ç”¨å·¥å…·æˆ–èµ„æºå‘ç”Ÿå˜åŒ–æ—¶è‡ªåŠ¨é€šçŸ¥
- **è¿›åº¦æ›´æ–°**ï¼šé•¿æ—¶é—´æ“ä½œçš„å®æ—¶è¿›åº¦åé¦ˆ
- **çŠ¶æ€åŒæ­¥**ï¼šä¿æŒå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨çŠ¶æ€ä¸€è‡´

è¿™ä½¿å¾— MCP åº”ç”¨èƒ½å¤ŸåŠ¨æ€å“åº”ç¯å¢ƒå˜åŒ–ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

---

## 5. å¼€å‘å®æˆ˜æŒ‡å—

### 5.1 ç¯å¢ƒæ­å»º

#### ğŸ Python å¼€å‘ç¯å¢ƒ

```bash
# 1. å®‰è£…ç°ä»£ Python åŒ…ç®¡ç†å™¨
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. åˆ›å»ºé¡¹ç›®
mkdir my-mcp-server && cd my-mcp-server
uv init --python=3.11

# 3. å®‰è£…ä¾èµ–
uv add "mcp[cli]" "fastapi" "pydantic" "aiofiles"

# 4. åˆ›å»ºé¡¹ç›®ç»“æ„
mkdir -p src/{server,client,tools,config}
touch src/server/__init__.py
touch src/tools/__init__.py
```

#### ğŸ“ æ¨èçš„é¡¹ç›®ç»“æ„

```
my-mcp-server/
â”œâ”€â”€ pyproject.toml              # é¡¹ç›®é…ç½®
â”œâ”€â”€ README.md                   # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ .env.example               # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ requirements.txt           # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server/                # æœåŠ¡å™¨å®ç°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py           # ä¸»æœåŠ¡å™¨é€»è¾‘
â”‚   â”‚   â””â”€â”€ config.py         # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ tools/                 # å·¥å…·å®ç°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_tools.py     # æ–‡ä»¶æ“ä½œå·¥å…·
â”‚   â”‚   â”œâ”€â”€ api_tools.py      # API é›†æˆå·¥å…·
â”‚   â”‚   â””â”€â”€ data_tools.py     # æ•°æ®å¤„ç†å·¥å…·
â”‚   â””â”€â”€ client/                # å®¢æˆ·ç«¯å·¥å…·
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_client.py    # æµ‹è¯•å®¢æˆ·ç«¯
â”œâ”€â”€ tests/                     # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ test_tools.py
â”‚   â””â”€â”€ test_server.py
â””â”€â”€ docs/                      # æ–‡æ¡£
    â”œâ”€â”€ api.md
    â””â”€â”€ examples.md
```

### 5.2 æ„å»ºé«˜çº§ MCP æœåŠ¡å™¨

#### ğŸ”§ å®Œæ•´çš„æ–‡ä»¶ç®¡ç†æœåŠ¡å™¨

```python
# src/server/main.py
import os
import json
import aiofiles
import asyncio
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

from mcp.server.fastmcp import FastMCP
from pydantic import BaseModel, validator

# åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
mcp = FastMCP("é«˜çº§æ–‡ä»¶ç®¡ç†æœåŠ¡å™¨")

class FileInfo(BaseModel):
    """æ–‡ä»¶ä¿¡æ¯æ¨¡å‹"""
    path: str
    name: str
    size: int
    modified: datetime
    is_directory: bool
    permissions: str

class SearchConfig(BaseModel):
    """æœç´¢é…ç½®æ¨¡å‹"""
    pattern: str
    directory: str = "."
    recursive: bool = True
    include_hidden: bool = False
    max_results: int = 100
    
    @validator('directory')
    def validate_directory(cls, v):
        if not os.path.exists(v):
            raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {v}")
        return v

# =============================================================================
# æ–‡ä»¶æ“ä½œå·¥å…·
# =============================================================================

@mcp.tool()
async def advanced_file_search(config: SearchConfig) -> str:
    """é«˜çº§æ–‡ä»¶æœç´¢åŠŸèƒ½
    
    Args:
        config: æœç´¢é…ç½®ï¼ŒåŒ…å«æ¨¡å¼ã€ç›®å½•ã€é€’å½’ç­‰é€‰é¡¹
        
    Returns:
        str: JSONæ ¼å¼çš„æœç´¢ç»“æœ
    """
    import glob
    
    try:
        search_pattern = os.path.join(config.directory, "**", config.pattern) if config.recursive else os.path.join(config.directory, config.pattern)
        
        files = glob.glob(search_pattern, recursive=config.recursive)
        
        # è¿‡æ»¤éšè—æ–‡ä»¶
        if not config.include_hidden:
            files = [f for f in files if not any(part.startswith('.') for part in Path(f).parts)]
        
        # è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        file_infos = []
        for file_path in files[:config.max_results]:
            try:
                stat = os.stat(file_path)
                info = FileInfo(
                    path=file_path,
                    name=os.path.basename(file_path),
                    size=stat.st_size,
                    modified=datetime.fromtimestamp(stat.st_mtime),
                    is_directory=os.path.isdir(file_path),
                    permissions=oct(stat.st_mode)[-3:]
                )
                file_infos.append(info.dict())
            except (OSError, PermissionError):
                continue
        
        result = {
            "query": config.dict(),
            "results_count": len(file_infos),
            "files": file_infos
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2, default=str)
        
    except Exception as e:
        return json.dumps({"error": str(e)}, ensure_ascii=False)

@mcp.tool()
async def batch_file_operation(operation: str, file_paths: List[str], target_dir: Optional[str] = None) -> str:
    """æ‰¹é‡æ–‡ä»¶æ“ä½œ
    
    Args:
        operation: æ“ä½œç±»å‹ (copy, move, delete, compress)
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        target_dir: ç›®æ ‡ç›®å½• (copy/moveæ“ä½œéœ€è¦)
        
    Returns:
        str: æ“ä½œç»“æœæ‘˜è¦
    """
    import shutil
    import zipfile
    
    results = {
        "operation": operation,
        "total_files": len(file_paths),
        "success": 0,
        "failed": 0,
        "errors": []
    }
    
    for file_path in file_paths:
        try:
            if operation == "copy" and target_dir:
                shutil.copy2(file_path, target_dir)
            elif operation == "move" and target_dir:
                shutil.move(file_path, target_dir)
            elif operation == "delete":
                if os.path.isdir(file_path):
                    shutil.rmtree(file_path)
                else:
                    os.remove(file_path)
            elif operation == "compress" and target_dir:
                zip_path = os.path.join(target_dir, f"archive_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip")
                with zipfile.ZipFile(zip_path, 'w') as zf:
                    for fp in file_paths:
                        if os.path.exists(fp):
                            zf.write(fp, os.path.basename(fp))
                break  # å‹ç¼©æ“ä½œåªéœ€è¦æ‰§è¡Œä¸€æ¬¡
            
            results["success"] += 1
            
        except Exception as e:
            results["failed"] += 1
            results["errors"].append(f"{file_path}: {str(e)}")
    
    return json.dumps(results, ensure_ascii=False, indent=2)

@mcp.tool()
async def analyze_directory(directory: str) -> str:
    """åˆ†æç›®å½•ç»“æ„å’Œç»Ÿè®¡ä¿¡æ¯
    
    Args:
        directory: è¦åˆ†æçš„ç›®å½•è·¯å¾„
        
    Returns:
        str: ç›®å½•åˆ†ææŠ¥å‘Š
    """
    if not os.path.exists(directory):
        return json.dumps({"error": "ç›®å½•ä¸å­˜åœ¨"}, ensure_ascii=False)
    
    analysis = {
        "directory": directory,
        "total_files": 0,
        "total_directories": 0,
        "total_size": 0,
        "file_types": {},
        "largest_files": [],
        "newest_files": [],
        "directory_tree": {}
    }
    
    # éå†ç›®å½•
    for root, dirs, files in os.walk(directory):
        analysis["total_directories"] += len(dirs)
        analysis["total_files"] += len(files)
        
        for file in files:
            file_path = os.path.join(root, file)
            try:
                stat = os.stat(file_path)
                file_size = stat.st_size
                file_ext = os.path.splitext(file)[1].lower()
                
                analysis["total_size"] += file_size
                
                # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
                analysis["file_types"][file_ext] = analysis["file_types"].get(file_ext, 0) + 1
                
                # æœ€å¤§æ–‡ä»¶è®°å½•
                file_info = {
                    "path": file_path,
                    "size": file_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
                }
                
                analysis["largest_files"].append(file_info)
                analysis["newest_files"].append(file_info)
                
            except (OSError, PermissionError):
                continue
    
    # æ’åºå¹¶é™åˆ¶æ•°é‡
    analysis["largest_files"] = sorted(analysis["largest_files"], key=lambda x: x["size"], reverse=True)[:10]
    analysis["newest_files"] = sorted(analysis["newest_files"], key=lambda x: x["modified"], reverse=True)[:10]
    
    # æ ¼å¼åŒ–å¤§å°
    def format_size(size_bytes):
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} TB"
    
    analysis["total_size_formatted"] = format_size(analysis["total_size"])
    
    return json.dumps(analysis, ensure_ascii=False, indent=2, default=str)

# =============================================================================
# èµ„æºå®šä¹‰
# =============================================================================

@mcp.resource("config://file-server")
async def get_server_config() -> str:
    """è·å–æ–‡ä»¶æœåŠ¡å™¨é…ç½®"""
    config = {
        "server_name": "é«˜çº§æ–‡ä»¶ç®¡ç†æœåŠ¡å™¨",
        "version": "2.0.0",
        "supported_operations": [
            "search", "copy", "move", "delete", 
            "compress", "analyze", "monitor"
        ],
        "limits": {
            "max_file_size": "100MB",
            "max_search_results": 1000,
            "max_batch_operations": 50
        },
        "security": {
            "allowed_paths": ["/home", "/tmp", "/var/log"],
            "forbidden_extensions": [".exe", ".bat", ".cmd"],
            "require_confirmation": ["delete", "move"]
        }
    }
    
    return json.dumps(config, ensure_ascii=False, indent=2)

@mcp.resource("stats://usage")
async def get_usage_stats() -> str:
    """è·å–æœåŠ¡å™¨ä½¿ç”¨ç»Ÿè®¡"""
    # è¿™é‡Œå¯ä»¥ä»æ•°æ®åº“æˆ–æ—¥å¿—æ–‡ä»¶è¯»å–å®é™…ç»Ÿè®¡æ•°æ®
    stats = {
        "total_requests": 1247,
        "successful_operations": 1198,
        "failed_operations": 49,
        "most_used_tools": [
            {"name": "advanced_file_search", "count": 456},
            {"name": "batch_file_operation", "count": 342},
            {"name": "analyze_directory", "count": 234}
        ],
        "uptime": "7 days, 12 hours",
        "last_updated": datetime.now().isoformat()
    }
    
    return json.dumps(stats, ensure_ascii=False, indent=2)

# =============================================================================
# æç¤ºæ¨¡æ¿
# =============================================================================

@mcp.prompt()
def file_organization_prompt(directory: str, strategy: str = "type") -> str:
    """æ–‡ä»¶æ•´ç†ç­–ç•¥æç¤ºæ¨¡æ¿"""
    
    strategies = {
        "type": "æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç±» (å›¾ç‰‡ã€æ–‡æ¡£ã€è§†é¢‘ç­‰)",
        "date": "æŒ‰ä¿®æ”¹æ—¥æœŸåˆ†ç±» (å¹´/æœˆ/æ—¥ ç»“æ„)",
        "size": "æŒ‰æ–‡ä»¶å¤§å°åˆ†ç±» (å°ã€ä¸­ã€å¤§æ–‡ä»¶)",
        "project": "æŒ‰é¡¹ç›®æˆ–ä¸»é¢˜åˆ†ç±»"
    }
    
    return f"""
è¯·å¸®æˆ‘æ•´ç† "{directory}" ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œä½¿ç”¨ "{strategy}" ç­–ç•¥ã€‚

æ•´ç†ç­–ç•¥è¯´æ˜ï¼š{strategies.get(strategy, strategy)}

è¦æ±‚ï¼š
1. åˆ†æç°æœ‰æ–‡ä»¶ç»“æ„å’Œå†…å®¹
2. æå‡ºå…·ä½“çš„æ•´ç†æ–¹æ¡ˆ
3. è¯´æ˜æ¯ä¸ªæ–‡ä»¶åº”è¯¥ç§»åŠ¨åˆ°å“ªä¸ªå­ç›®å½•
4. åˆ›å»ºå¿…è¦çš„å­ç›®å½•ç»“æ„
5. ç¡®ä¿æ•´ç†åä¾¿äºæŸ¥æ‰¾å’Œç®¡ç†

è¯·å…ˆä½¿ç”¨ analyze_directory å·¥å…·åˆ†æç›®å½•æƒ…å†µï¼Œç„¶åæå‡ºæ•´ç†å»ºè®®ã€‚
"""

# =============================================================================
# æœåŠ¡å™¨å¯åŠ¨
# =============================================================================

if __name__ == "__main__":
    # å¯ä»¥æ·»åŠ å‘½ä»¤è¡Œå‚æ•°å¤„ç†
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description="é«˜çº§æ–‡ä»¶ç®¡ç† MCP æœåŠ¡å™¨")
    parser.add_argument("--host", default="localhost", help="æœåŠ¡å™¨ä¸»æœº")
    parser.add_argument("--port", type=int, default=8080, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--transport", choices=["stdio", "sse"], default="stdio", help="ä¼ è¾“æ–¹å¼")
    
    args = parser.parse_args()
    
    if args.transport == "stdio":
        # STDIO æ¨¡å¼ (æœ¬åœ°ä½¿ç”¨)
        mcp.run()
    else:
        # HTTP/SSE æ¨¡å¼ (è¿œç¨‹ä½¿ç”¨)
        from mcp.server.sse import SseServerTransport
        
        async def main():
            transport = SseServerTransport("/sse")
            async with mcp.run_server() as server:
                await transport.run(server, host=args.host, port=args.port)
        
        asyncio.run(main())
```

### 5.3 å®¢æˆ·ç«¯é…ç½®å’Œé›†æˆ

#### âš™ï¸ Claude Desktop é…ç½®

```json
// ~/.config/claude-desktop/claude_desktop_config.json
{
  "mcpServers": {
    "advanced-file-server": {
      "command": "uv",
      "args": [
        "--directory", "/path/to/your/project",
        "run", "python", "src/server/main.py"
      ],
      "env": {
        "LOG_LEVEL": "INFO"
      }
    },
    "database-server": {
      "command": "docker",
      "args": [
        "run", "-i", "--rm",
        "-v", "/data:/data",
        "my-mcp-db-server"
      ]
    },
    "remote-api-server": {
      "command": "curl",
      "args": [
        "-X", "POST",
        "-H", "Content-Type: application/json",
        "https://api.example.com/mcp"
      ],
      "env": {
        "API_KEY": "${API_KEY}"
      }
    }
  }
}
```

#### ğŸ§ª å¼€å‘å’Œæµ‹è¯•å·¥å…·

```python
# src/client/test_client.py
import asyncio
import json
from mcp.client import StdioMCPClient

class MCPTester:
    """MCP æœåŠ¡å™¨æµ‹è¯•å·¥å…·"""
    
    def __init__(self, server_command: str):
        self.server_command = server_command
        self.client = None
    
    async def connect(self):
        """è¿æ¥åˆ° MCP æœåŠ¡å™¨"""
        self.client = StdioMCPClient(self.server_command)
        await self.client.connect()
        
        # åˆå§‹åŒ–
        result = await self.client.initialize()
        print(f"æœåŠ¡å™¨ä¿¡æ¯: {result['serverInfo']}")
        
        return result
    
    async def test_tools(self):
        """æµ‹è¯•æ‰€æœ‰å¯ç”¨å·¥å…·"""
        # è·å–å·¥å…·åˆ—è¡¨
        tools = await self.client.list_tools()
        print(f"å‘ç° {len(tools)} ä¸ªå·¥å…·:")
        
        for tool in tools:
            print(f"- {tool['name']}: {tool['description']}")
        
        # æµ‹è¯•æ–‡ä»¶æœç´¢å·¥å…·
        search_result = await self.client.call_tool(
            "advanced_file_search",
            {
                "config": {
                    "pattern": "*.py",
                    "directory": ".",
                    "recursive": True,
                    "max_results": 5
                }
            }
        )
        
        print("æœç´¢ç»“æœ:")
        print(json.dumps(json.loads(search_result), indent=2, ensure_ascii=False))
    
    async def test_resources(self):
        """æµ‹è¯•æ‰€æœ‰å¯ç”¨èµ„æº"""
        resources = await self.client.list_resources()
        print(f"å‘ç° {len(resources)} ä¸ªèµ„æº:")
        
        for resource in resources:
            print(f"- {resource['uri']}: {resource['name']}")
            
            # è¯»å–èµ„æºå†…å®¹
            content = await self.client.read_resource(resource['uri'])
            print(f"  å†…å®¹é¢„è§ˆ: {content[:100]}...")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            await self.client.close()

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    tester = MCPTester("python src/server/main.py")
    
    try:
        await tester.connect()
        await tester.test_tools()
        await tester.test_resources()
    finally:
        await tester.close()

if __name__ == "__main__":
    asyncio.run(main())
```

#### ğŸ” MCP Inspector ä½¿ç”¨

```bash
# å¯åŠ¨è°ƒè¯•å·¥å…·
mcp dev src/server/main.py

# è®¿é—® http://localhost:5173
# åŠŸèƒ½åŒ…æ‹¬ï¼š
# - å®æ—¶æŸ¥çœ‹å·¥å…·åˆ—è¡¨
# - äº¤äº’å¼æµ‹è¯•å·¥å…·è°ƒç”¨  
# - å‚æ•°éªŒè¯å’Œé”™è¯¯æ£€æŸ¥
# - æŸ¥çœ‹èµ„æºå’Œæç¤ºæ¨¡æ¿
# - ç›‘æ§è¿æ¥çŠ¶æ€å’Œæ—¥å¿—
```

---

## 6. å®é™…åº”ç”¨æ¡ˆä¾‹

### 6.1 æ™ºèƒ½å¼€å‘åŠ©æ‰‹

#### ğŸš€ åœºæ™¯ï¼šAI é©±åŠ¨çš„ä»£ç å®¡æŸ¥ç³»ç»Ÿ

```python
# å®Œæ•´çš„ä»£ç å®¡æŸ¥ MCP æœåŠ¡å™¨
from mcp.server.fastmcp import FastMCP
import git
import ast
import subprocess
import json
from typing import List, Dict

mcp = FastMCP("æ™ºèƒ½ä»£ç å®¡æŸ¥åŠ©æ‰‹")

@mcp.tool()
async def review_pull_request(repo_path: str, pr_number: int, focus_areas: List[str]) -> str:
    """å…¨é¢çš„ä»£ç å®¡æŸ¥å·¥å…·
    
    Args:
        repo_path: Git ä»“åº“è·¯å¾„
        pr_number: Pull Request ç¼–å·  
        focus_areas: å®¡æŸ¥é‡ç‚¹ (security, performance, style, logic)
    """
    
    try:
        # 1. è·å– PR å˜æ›´
        repo = git.Repo(repo_path)
        
        # è·å– PR çš„å˜æ›´æ–‡ä»¶
        changed_files = []
        for item in repo.index.diff('HEAD~1'):
            if item.a_path.endswith(('.py', '.js', '.ts', '.java', '.cpp')):
                changed_files.append({
                    'file': item.a_path,
                    'change_type': item.change_type,
                    'diff': get_file_diff(repo, item.a_path)
                })
        
        # 2. åˆ†æä»£ç è´¨é‡
        analysis_results = {
            'summary': {
                'total_files': len(changed_files),
                'lines_added': 0,
                'lines_removed': 0,
                'risk_level': 'low'
            },
            'issues': [],
            'suggestions': [],
            'security_concerns': [],
            'performance_notes': []
        }
        
        for file_info in changed_files:
            file_analysis = await analyze_code_file(
                file_info['file'], 
                file_info['diff'], 
                focus_areas
            )
            
            # åˆå¹¶åˆ†æç»“æœ
            analysis_results['issues'].extend(file_analysis['issues'])
            analysis_results['suggestions'].extend(file_analysis['suggestions'])
            
            if 'security' in focus_areas:
                security_issues = await check_security_vulnerabilities(file_info)
                analysis_results['security_concerns'].extend(security_issues)
            
            if 'performance' in focus_areas:
                perf_issues = await analyze_performance(file_info)
                analysis_results['performance_notes'].extend(perf_issues)
        
        # 3. ç”Ÿæˆå®¡æŸ¥æŠ¥å‘Š
        report = generate_review_report(analysis_results, focus_areas)
        
        return json.dumps(report, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'å®¡æŸ¥å¤±è´¥: {str(e)}'}, ensure_ascii=False)

@mcp.tool()
async def run_automated_tests(repo_path: str, test_type: str = "all") -> str:
    """è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶
    
    Args:
        repo_path: ä»“åº“è·¯å¾„
        test_type: æµ‹è¯•ç±»å‹ (unit, integration, e2e, all)
    """
    
    test_commands = {
        'unit': ['python', '-m', 'pytest', 'tests/unit/', '-v'],
        'integration': ['python', '-m', 'pytest', 'tests/integration/', '-v'],
        'e2e': ['python', '-m', 'pytest', 'tests/e2e/', '-v'],
        'all': ['python', '-m', 'pytest', '-v', '--cov=src', '--cov-report=json']
    }
    
    if test_type not in test_commands:
        return json.dumps({'error': f'ä¸æ”¯æŒçš„æµ‹è¯•ç±»å‹: {test_type}'})
    
    try:
        result = subprocess.run(
            test_commands[test_type],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        test_report = {
            'test_type': test_type,
            'exit_code': result.returncode,
            'stdout': result.stdout,
            'stderr': result.stderr,
            'success': result.returncode == 0
        }
        
        # è§£ææµ‹è¯•ç»“æœ
        if test_type == 'all':
            coverage_data = parse_coverage_report(repo_path)
            test_report['coverage'] = coverage_data
        
        return json.dumps(test_report, ensure_ascii=False, indent=2)
        
    except subprocess.TimeoutExpired:
        return json.dumps({'error': 'æµ‹è¯•è¶…æ—¶'}, ensure_ascii=False)
    except Exception as e:
        return json.dumps({'error': f'æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}'}, ensure_ascii=False)

@mcp.tool()
async def suggest_code_improvements(file_path: str, language: str) -> str:
    """åŸºäºæœ€ä½³å®è·µå»ºè®®ä»£ç æ”¹è¿›
    
    Args:
        file_path: ä»£ç æ–‡ä»¶è·¯å¾„
        language: ç¼–ç¨‹è¯­è¨€
    """
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            code_content = f.read()
        
        suggestions = []
        
        if language == 'python':
            suggestions = analyze_python_code(code_content)
        elif language in ['javascript', 'typescript']:
            suggestions = analyze_js_code(code_content)
        elif language == 'java':
            suggestions = analyze_java_code(code_content)
        
        improvement_report = {
            'file': file_path,
            'language': language,
            'suggestions': suggestions,
            'refactoring_opportunities': find_refactoring_opportunities(code_content, language),
            'best_practices': get_language_best_practices(language)
        }
        
        return json.dumps(improvement_report, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'åˆ†æå¤±è´¥: {str(e)}'}, ensure_ascii=False)

# è¾…åŠ©å‡½æ•°
async def analyze_code_file(file_path: str, diff: str, focus_areas: List[str]) -> Dict:
    """åˆ†æå•ä¸ªä»£ç æ–‡ä»¶"""
    
    issues = []
    suggestions = []
    
    # åŸºç¡€è¯­æ³•å’Œé£æ ¼æ£€æŸ¥
    if 'style' in focus_areas:
        style_issues = check_code_style(file_path, diff)
        issues.extend(style_issues)
    
    # é€»è¾‘å¤æ‚åº¦åˆ†æ
    if 'logic' in focus_areas:
        complexity_issues = analyze_complexity(file_path, diff)
        suggestions.extend(complexity_issues)
    
    return {
        'file': file_path,
        'issues': issues,
        'suggestions': suggestions
    }

def analyze_python_code(code: str) -> List[Dict]:
    """åˆ†æ Python ä»£ç è´¨é‡"""
    
    suggestions = []
    
    try:
        tree = ast.parse(code)
        
        # æ£€æŸ¥å‡½æ•°é•¿åº¦
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_lines = node.end_lineno - node.lineno + 1
                if func_lines > 50:
                    suggestions.append({
                        'type': 'refactoring',
                        'line': node.lineno,
                        'message': f'å‡½æ•° {node.name} å¤ªé•¿ ({func_lines} è¡Œ)ï¼Œå»ºè®®æ‹†åˆ†',
                        'severity': 'medium'
                    })
                
                # æ£€æŸ¥å‚æ•°æ•°é‡
                if len(node.args.args) > 5:
                    suggestions.append({
                        'type': 'design',
                        'line': node.lineno,
                        'message': f'å‡½æ•° {node.name} å‚æ•°è¿‡å¤šï¼Œè€ƒè™‘ä½¿ç”¨é…ç½®å¯¹è±¡',
                        'severity': 'low'
                    })
        
        # æ£€æŸ¥ç±»è®¾è®¡
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                methods = [n for n in node.body if isinstance(n, ast.FunctionDef)]
                if len(methods) > 20:
                    suggestions.append({
                        'type': 'design',
                        'line': node.lineno,
                        'message': f'ç±» {node.name} æ–¹æ³•è¿‡å¤šï¼Œè¿åå•ä¸€èŒè´£åŸåˆ™',
                        'severity': 'high'
                    })
        
    except SyntaxError as e:
        suggestions.append({
            'type': 'error',
            'line': e.lineno,
            'message': f'è¯­æ³•é”™è¯¯: {e.msg}',
            'severity': 'critical'
        })
    
    return suggestions

@mcp.resource("templates://code-review")
async def get_review_templates() -> str:
    """è·å–ä»£ç å®¡æŸ¥æ¨¡æ¿"""
    
    templates = {
        'pull_request_checklist': [
            'âœ… ä»£ç é£æ ¼ç¬¦åˆé¡¹ç›®è§„èŒƒ',
            'âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡',
            'âœ… ä»£ç è¦†ç›–ç‡ä¸ä½äº80%',
            'âœ… æ— å®‰å…¨æ¼æ´',
            'âœ… æ€§èƒ½æ— æ˜æ˜¾ä¸‹é™',
            'âœ… æ–‡æ¡£å·²æ›´æ–°',
            'âœ… å‘åå…¼å®¹æ€§æ£€æŸ¥'
        ],
        'review_comments': {
            'naming': 'å»ºè®®ä½¿ç”¨æ›´æœ‰æ„ä¹‰çš„å˜é‡åï¼Œå½“å‰åç§°ä¸å¤Ÿæ¸…æ™°',
            'complexity': 'è¿™ä¸ªå‡½æ•°é€»è¾‘å¤æ‚ï¼Œå»ºè®®æ‹†åˆ†ä¸ºæ›´å°çš„å‡½æ•°',
            'error_handling': 'ç¼ºå°‘é”™è¯¯å¤„ç†ï¼Œåº”è¯¥æ·»åŠ  try-catch å—',
            'performance': 'è¿™ä¸ªå®ç°å¯èƒ½æœ‰æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®ä¼˜åŒ–',
            'security': 'å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œéœ€è¦æ·»åŠ è¾“å…¥éªŒè¯'
        },
        'improvement_suggestions': [
            'è€ƒè™‘ä½¿ç”¨è®¾è®¡æ¨¡å¼ç®€åŒ–ä»£ç ç»“æ„',
            'æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›–æ–°åŠŸèƒ½',
            'è€ƒè™‘æ·»åŠ æ—¥å¿—è®°å½•ä¾¿äºè°ƒè¯•',
            'è¯„ä¼°æ˜¯å¦éœ€è¦æ·»åŠ æ€§èƒ½ç›‘æ§'
        ]
    }
    
    return json.dumps(templates, ensure_ascii=False, indent=2)
```

### 6.2 æ•°æ®åˆ†æè‡ªåŠ¨åŒ–

#### ğŸ“Š åœºæ™¯ï¼šæ™ºèƒ½æ•°æ®åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ

```python
# æ•°æ®åˆ†æ MCP æœåŠ¡å™¨
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sqlalchemy import create_engine
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta

mcp = FastMCP("æ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹")

@mcp.tool()
async def analyze_sales_data(data_source: str, time_period: str, metrics: List[str]) -> str:
    """å…¨é¢çš„é”€å”®æ•°æ®åˆ†æ
    
    Args:
        data_source: æ•°æ®æº (file_path æˆ– database_url)
        time_period: æ—¶é—´èŒƒå›´ (last_30_days, last_quarter, last_year)
        metrics: åˆ†ææŒ‡æ ‡ (revenue, growth, conversion, retention)
    """
    
    try:
        # 1. åŠ è½½æ•°æ®
        if data_source.startswith('postgresql://'):
            engine = create_engine(data_source)
            df = pd.read_sql(get_sales_query(time_period), engine)
        else:
            df = pd.read_csv(data_source)
            df = filter_by_time_period(df, time_period)
        
        # 2. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
        df = clean_sales_data(df)
        
        # 3. ç”Ÿæˆåˆ†æç»“æœ
        analysis_results = {
            'data_summary': {
                'total_records': len(df),
                'date_range': {
                    'start': df['date'].min().isoformat(),
                    'end': df['date'].max().isoformat()
                },
                'total_revenue': float(df['amount'].sum()),
                'average_order_value': float(df['amount'].mean())
            },
            'metrics': {}
        }
        
        # 4. æŒ‰éœ€æ±‚è®¡ç®—æŒ‡æ ‡
        if 'revenue' in metrics:
            analysis_results['metrics']['revenue'] = analyze_revenue_trends(df)
        
        if 'growth' in metrics:
            analysis_results['metrics']['growth'] = calculate_growth_metrics(df)
        
        if 'conversion' in metrics:
            analysis_results['metrics']['conversion'] = analyze_conversion_funnel(df)
        
        if 'retention' in metrics:
            analysis_results['metrics']['retention'] = calculate_retention_rates(df)
        
        # 5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        charts = generate_sales_charts(df, metrics)
        analysis_results['charts'] = charts
        
        # 6. ç”Ÿæˆæ´å¯Ÿå’Œå»ºè®®
        insights = generate_business_insights(analysis_results)
        analysis_results['insights'] = insights
        
        return json.dumps(analysis_results, ensure_ascii=False, indent=2, default=str)
        
    except Exception as e:
        return json.dumps({'error': f'åˆ†æå¤±è´¥: {str(e)}'}, ensure_ascii=False)

@mcp.tool()
async def create_dashboard(data_source: str, dashboard_type: str, filters: Dict) -> str:
    """åˆ›å»ºäº¤äº’å¼æ•°æ®ä»ªè¡¨æ¿
    
    Args:
        data_source: æ•°æ®æº
        dashboard_type: ä»ªè¡¨æ¿ç±»å‹ (sales, marketing, operations, financial)
        filters: è¿‡æ»¤æ¡ä»¶
    """
    
    try:
        # åŠ è½½æ•°æ®
        df = load_data(data_source, filters)
        
        # æ ¹æ®ç±»å‹ç”Ÿæˆä¸åŒçš„ä»ªè¡¨æ¿
        if dashboard_type == 'sales':
            dashboard_html = create_sales_dashboard(df)
        elif dashboard_type == 'marketing':
            dashboard_html = create_marketing_dashboard(df)
        elif dashboard_type == 'operations':
            dashboard_html = create_operations_dashboard(df)
        elif dashboard_type == 'financial':
            dashboard_html = create_financial_dashboard(df)
        else:
            return json.dumps({'error': f'ä¸æ”¯æŒçš„ä»ªè¡¨æ¿ç±»å‹: {dashboard_type}'})
        
        # ä¿å­˜ä»ªè¡¨æ¿æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{dashboard_type}_dashboard_{timestamp}.html"
        filepath = f"./dashboards/{filename}"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(dashboard_html)
        
        result = {
            'dashboard_type': dashboard_type,
            'file_path': filepath,
            'preview_url': f'file://{os.path.abspath(filepath)}',
            'created_at': datetime.now().isoformat(),
            'data_summary': {
                'records_count': len(df),
                'columns': list(df.columns),
                'date_range': get_date_range(df)
            }
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'ä»ªè¡¨æ¿åˆ›å»ºå¤±è´¥: {str(e)}'}, ensure_ascii=False)

@mcp.tool()
async def predict_trends(data_source: str, target_column: str, prediction_days: int = 30) -> str:
    """åŸºäºå†å²æ•°æ®é¢„æµ‹è¶‹åŠ¿
    
    Args:
        data_source: æ•°æ®æº
        target_column: ç›®æ ‡é¢„æµ‹åˆ—
        prediction_days: é¢„æµ‹å¤©æ•°
    """
    
    try:
        from sklearn.linear_model import LinearRegression
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.metrics import mean_absolute_error, r2_score
        
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        df = load_data(data_source)
        df = prepare_time_series_data(df, target_column)
        
        # ç‰¹å¾å·¥ç¨‹
        features = create_time_features(df)
        X = features[:-prediction_days]  # è®­ç»ƒæ•°æ®
        y = df[target_column][:-prediction_days]  # ç›®æ ‡å˜é‡
        
        # è®­ç»ƒå¤šä¸ªæ¨¡å‹
        models = {
            'linear_regression': LinearRegression(),
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42)
        }
        
        model_results = {}
        best_model = None
        best_score = float('-inf')
        
        for name, model in models.items():
            # è®­ç»ƒæ¨¡å‹
            model.fit(X, y)
            
            # è¯„ä¼°æ¨¡å‹
            y_pred = model.predict(X)
            mae = mean_absolute_error(y, y_pred)
            r2 = r2_score(y, y_pred)
            
            model_results[name] = {
                'mae': float(mae),
                'r2_score': float(r2),
                'model': model
            }
            
            if r2 > best_score:
                best_score = r2
                best_model = model
        
        # ç”Ÿæˆé¢„æµ‹
        future_features = features[-prediction_days:]
        predictions = best_model.predict(future_features)
        
        # åˆ›å»ºé¢„æµ‹ç»“æœ
        future_dates = pd.date_range(
            start=df.index[-1] + timedelta(days=1),
            periods=prediction_days,
            freq='D'
        )
        
        prediction_results = {
            'model_performance': model_results,
            'best_model': max(model_results.keys(), key=lambda k: model_results[k]['r2_score']),
            'predictions': {
                'dates': [d.isoformat() for d in future_dates],
                'values': predictions.tolist(),
                'confidence_interval': calculate_prediction_intervals(predictions)
            },
            'historical_data': {
                'dates': [d.isoformat() for d in df.index[-30:]],
                'values': df[target_column][-30:].tolist()
            },
            'trend_analysis': analyze_trend_direction(predictions)
        }
        
        # ç”Ÿæˆé¢„æµ‹å›¾è¡¨
        chart_html = create_prediction_chart(df, predictions, future_dates, target_column)
        prediction_results['chart_html'] = chart_html
        
        return json.dumps(prediction_results, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'}, ensure_ascii=False)

# è¾…åŠ©å‡½æ•°
def create_sales_dashboard(df: pd.DataFrame) -> str:
    """åˆ›å»ºé”€å”®ä»ªè¡¨æ¿"""
    
    # åˆ›å»ºå¤šä¸ªå›¾è¡¨
    fig = go.Figure()
    
    # æ”¶å…¥è¶‹åŠ¿å›¾
    daily_revenue = df.groupby('date')['amount'].sum()
    fig.add_trace(go.Scatter(
        x=daily_revenue.index,
        y=daily_revenue.values,
        mode='lines+markers',
        name='æ¯æ—¥æ”¶å…¥',
        line=dict(color='#1f77b4', width=2)
    ))
    
    # è®¢å•æ•°é‡è¶‹åŠ¿
    daily_orders = df.groupby('date').size()
    fig.add_trace(go.Scatter(
        x=daily_orders.index,
        y=daily_orders.values,
        mode='lines+markers',
        name='æ¯æ—¥è®¢å•æ•°',
        yaxis='y2',
        line=dict(color='#ff7f0e', width=2)
    ))
    
    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        title='é”€å”®ä»ªè¡¨æ¿',
        xaxis_title='æ—¥æœŸ',
        yaxis=dict(title='æ”¶å…¥ (å…ƒ)', side='left'),
        yaxis2=dict(title='è®¢å•æ•°', side='right', overlaying='y'),
        hovermode='x unified',
        template='plotly_white'
    )
    
    # ç”Ÿæˆå®Œæ•´çš„ HTML
    dashboard_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>é”€å”®æ•°æ®ä»ªè¡¨æ¿</title>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .dashboard-header {{ text-align: center; margin-bottom: 30px; }}
            .metrics-grid {{ display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; margin-bottom: 30px; }}
            .metric-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; }}
            .metric-value {{ font-size: 2em; font-weight: bold; color: #28a745; }}
            .metric-label {{ color: #6c757d; margin-top: 5px; }}
        </style>
    </head>
    <body>
        <div class="dashboard-header">
            <h1>ğŸ“Š é”€å”®æ•°æ®ä»ªè¡¨æ¿</h1>
            <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="metrics-grid">
            <div class="metric-card">
                <div class="metric-value">{df['amount'].sum():,.0f}</div>
                <div class="metric-label">æ€»æ”¶å…¥ (å…ƒ)</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{len(df):,}</div>
                <div class="metric-label">æ€»è®¢å•æ•°</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{df['amount'].mean():,.0f}</div>
                <div class="metric-label">å¹³å‡è®¢å•é‡‘é¢</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{df.groupby('date').size().mean():.1f}</div>
                <div class="metric-label">æ—¥å‡è®¢å•æ•°</div>
            </div>
        </div>
        
        <div id="chart" style="height: 600px;"></div>
        
        <script>
            {fig.to_html(include_plotlyjs=False, div_id="chart")}
        </script>
    </body>
    </html>
    """
    
    return dashboard_html

@mcp.resource("templates://analysis")
async def get_analysis_templates() -> str:
    """è·å–æ•°æ®åˆ†ææ¨¡æ¿"""
    
    templates = {
        'sql_queries': {
            'sales_overview': """
                SELECT 
                    DATE(created_at) as date,
                    COUNT(*) as order_count,
                    SUM(amount) as total_revenue,
                    AVG(amount) as avg_order_value
                FROM orders 
                WHERE created_at >= NOW() - INTERVAL '{days} days'
                GROUP BY DATE(created_at)
                ORDER BY date
            """,
            'customer_analysis': """
                SELECT 
                    customer_id,
                    COUNT(*) as order_count,
                    SUM(amount) as total_spent,
                    AVG(amount) as avg_order_value,
                    MAX(created_at) as last_order_date
                FROM orders
                GROUP BY customer_id
                HAVING COUNT(*) > 1
                ORDER BY total_spent DESC
            """
        },
        'report_sections': [
            'ğŸ“ˆ å…³é”®æŒ‡æ ‡æ¦‚è§ˆ',
            'ğŸ“Š è¶‹åŠ¿åˆ†æ',
            'ğŸ” æ·±åº¦æ´å¯Ÿ',
            'ğŸ’¡ è¡ŒåŠ¨å»ºè®®',
            'ğŸ“‹ é™„å½•æ•°æ®'
        ],
        'chart_types': {
            'time_series': ['line', 'area', 'bar'],
            'comparison': ['bar', 'column', 'radar'],
            'distribution': ['histogram', 'box', 'violin'],
            'relationship': ['scatter', 'heatmap', 'bubble']
        }
    }
    
    return json.dumps(templates, ensure_ascii=False, indent=2)
```

### 6.3 æ™ºèƒ½å®¢æœç³»ç»Ÿ

#### ğŸ¤– åœºæ™¯ï¼šåŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½å®¢æœ

```python
# æ™ºèƒ½å®¢æœ MCP æœåŠ¡å™¨
import json
import sqlite3
from datetime import datetime
from typing import List, Dict, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba

mcp = FastMCP("æ™ºèƒ½å®¢æœåŠ©æ‰‹")

class KnowledgeBase:
    """çŸ¥è¯†åº“ç®¡ç†ç±»"""
    
    def __init__(self, db_path: str = "knowledge_base.db"):
        self.db_path = db_path
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.init_database()
        self.load_knowledge()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                category TEXT NOT NULL,
                tags TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                view_count INTEGER DEFAULT 0,
                helpful_count INTEGER DEFAULT 0
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS customer_conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                customer_id TEXT NOT NULL,
                question TEXT NOT NULL,
                answer TEXT NOT NULL,
                satisfaction_score INTEGER,
                resolved BOOLEAN DEFAULT FALSE,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def load_knowledge(self):
        """åŠ è½½çŸ¥è¯†åº“å†…å®¹"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT id, title, content, category FROM knowledge_articles")
        self.articles = cursor.fetchall()
        
        if self.articles:
            # æ„å»ºæœç´¢ç´¢å¼•
            article_texts = [f"{article[1]} {article[2]}" for article in self.articles]
            self.vectorizer.fit(article_texts)
            self.article_vectors = self.vectorizer.transform(article_texts)
        
        conn.close()

kb = KnowledgeBase()

@mcp.tool()
async def search_knowledge_base(query: str, category: Optional[str] = None, top_k: int = 5) -> str:
    """æœç´¢çŸ¥è¯†åº“
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        category: å¯é€‰çš„ç±»åˆ«è¿‡æ»¤
        top_k: è¿”å›æœ€ç›¸å…³çš„kä¸ªç»“æœ
    """
    
    try:
        if not kb.articles:
            return json.dumps({'error': 'çŸ¥è¯†åº“ä¸ºç©º'}, ensure_ascii=False)
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
        query_vector = kb.vectorizer.transform([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, kb.article_vectors)[0]
        
        # è·å–æœ€ç›¸å…³çš„æ–‡ç« 
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                article = kb.articles[idx]
                
                # ç±»åˆ«è¿‡æ»¤
                if category and article[3] != category:
                    continue
                
                results.append({
                    'id': article[0],
                    'title': article[1],
                    'content': article[2][:300] + '...' if len(article[2]) > 300 else article[2],
                    'category': article[3],
                    'similarity_score': float(similarities[idx])
                })
        
        return json.dumps({
            'query': query,
            'results_count': len(results),
            'results': results
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'æœç´¢å¤±è´¥: {str(e)}'}, ensure_ascii=False)

@mcp.tool()
async def answer_customer_question(question: str, customer_id: str, context: Optional[str] = None) -> str:
    """å›ç­”å®¢æˆ·é—®é¢˜
    
    Args:
        question: å®¢æˆ·é—®é¢˜
        customer_id: å®¢æˆ·ID
        context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    
    try:
        # 1. æœç´¢ç›¸å…³çŸ¥è¯†
        knowledge_results = await search_knowledge_base(question, top_k=3)
        knowledge_data = json.loads(knowledge_results)
        
        # 2. åˆ†æé—®é¢˜æ„å›¾
        intent = analyze_question_intent(question)
        
        # 3. ç”Ÿæˆå›ç­”
        if knowledge_data['results']:
            # åŸºäºçŸ¥è¯†åº“ç”Ÿæˆå›ç­”
            best_match = knowledge_data['results'][0]
            
            if best_match['similarity_score'] > 0.7:
                # é«˜ç›¸ä¼¼åº¦ï¼Œç›´æ¥ä½¿ç”¨çŸ¥è¯†åº“å†…å®¹
                answer = generate_knowledge_based_answer(best_match, question)
                confidence = 'high'
            else:
                # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œéœ€è¦äººå·¥ç¡®è®¤
                answer = generate_suggested_answer(knowledge_data['results'], question)
                confidence = 'medium'
        else:
            # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çŸ¥è¯†ï¼Œå»ºè®®è½¬äººå·¥
            answer = generate_fallback_answer(question, intent)
            confidence = 'low'
        
        # 4. è®°å½•å¯¹è¯
        conversation_record = {
            'customer_id': customer_id,
            'question': question,
            'answer': answer,
            'intent': intent,
            'confidence': confidence,
            'knowledge_used': knowledge_data['results'][:1] if knowledge_data['results'] else [],
            'timestamp': datetime.now().isoformat()
        }
        
        save_conversation(conversation_record)
        
        response = {
            'answer': answer,
            'confidence': confidence,
            'intent': intent,
            'suggested_actions': get_suggested_actions(intent, confidence),
            'related_articles': knowledge_data['results'][:2]
        }
        
        return json.dumps(response, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}'}, ensure_ascii=False)

@mcp.tool()
async def escalate_to_human(conversation_id: str, reason: str, priority: str = "normal") -> str:
    """å‡çº§åˆ°äººå·¥å®¢æœ
    
    Args:
        conversation_id: å¯¹è¯ID
        reason: å‡çº§åŸå› 
        priority: ä¼˜å…ˆçº§ (low, normal, high, urgent)
    """
    
    try:
        # è·å–å¯¹è¯å†å²
        conversation_history = get_conversation_history(conversation_id)
        
        # åˆ›å»ºå‡çº§ç¥¨æ®
        ticket = {
            'id': f"TICKET_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            'conversation_id': conversation_id,
            'customer_id': conversation_history.get('customer_id'),
            'reason': reason,
            'priority': priority,
            'status': 'pending',
            'created_at': datetime.now().isoformat(),
            'conversation_summary': summarize_conversation(conversation_history),
            'suggested_solutions': get_suggested_solutions(conversation_history)
        }
        
        # ä¿å­˜ç¥¨æ®
        save_escalation_ticket(ticket)
        
        # é€šçŸ¥äººå·¥å®¢æœ
        notification_sent = notify_human_agents(ticket)
        
        response = {
            'ticket_id': ticket['id'],
            'estimated_wait_time': get_estimated_wait_time(priority),
            'status': 'escalated',
            'notification_sent': notification_sent,
            'next_steps': [
                'äººå·¥å®¢æœå°†åœ¨é¢„ä¼°æ—¶é—´å†…è”ç³»æ‚¨',
                'æ‚¨å¯ä»¥é€šè¿‡ç¥¨æ®IDæŸ¥è¯¢å¤„ç†è¿›åº¦',
                'å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·æ‹¨æ‰“å®¢æœçƒ­çº¿'
            ]
        }
        
        return json.dumps(response, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'å‡çº§å¤±è´¥: {str(e)}'}, ensure_ascii=False)

@mcp.tool()
async def generate_service_report(start_date: str, end_date: str, metrics: List[str]) -> str:
    """ç”Ÿæˆå®¢æœæœåŠ¡æŠ¥å‘Š
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD) 
        metrics: æŠ¥å‘ŠæŒ‡æ ‡ (resolution_rate, satisfaction, response_time, volume)
    """
    
    try:
        # æŸ¥è¯¢æ•°æ®
        conn = sqlite3.connect(kb.db_path)
        cursor = conn.cursor()
        
        # åŸºç¡€ç»Ÿè®¡
        cursor.execute('''
            SELECT COUNT(*) as total_conversations,
                   AVG(satisfaction_score) as avg_satisfaction,
                   COUNT(CASE WHEN resolved = 1 THEN 1 END) as resolved_count
            FROM customer_conversations 
            WHERE DATE(created_at) BETWEEN ? AND ?
        ''', (start_date, end_date))
        
        basic_stats = cursor.fetchone()
        
        report = {
            'period': {'start': start_date, 'end': end_date},
            'summary': {
                'total_conversations': basic_stats[0],
                'average_satisfaction': round(basic_stats[1] or 0, 2),
                'resolution_rate': round((basic_stats[2] / basic_stats[0] * 100) if basic_stats[0] > 0 else 0, 2)
            },
            'metrics': {}
        }
        
        # è¯¦ç»†æŒ‡æ ‡è®¡ç®—
        if 'volume' in metrics:
            cursor.execute('''
                SELECT DATE(created_at) as date, COUNT(*) as count
                FROM customer_conversations 
                WHERE DATE(created_at) BETWEEN ? AND ?
                GROUP BY DATE(created_at)
                ORDER BY date
            ''', (start_date, end_date))
            
            volume_data = cursor.fetchall()
            report['metrics']['daily_volume'] = [
                {'date': row[0], 'count': row[1]} for row in volume_data
            ]
        
        if 'satisfaction' in metrics:
            cursor.execute('''
                SELECT satisfaction_score, COUNT(*) as count
                FROM customer_conversations 
                WHERE DATE(created_at) BETWEEN ? AND ? 
                AND satisfaction_score IS NOT NULL
                GROUP BY satisfaction_score
                ORDER BY satisfaction_score
            ''', (start_date, end_date))
            
            satisfaction_data = cursor.fetchall()
            report['metrics']['satisfaction_distribution'] = [
                {'score': row[0], 'count': row[1]} for row in satisfaction_data
            ]
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        insights = generate_service_insights(report)
        report['insights'] = insights
        
        conn.close()
        
        return json.dumps(report, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}'}, ensure_ascii=False)

# è¾…åŠ©å‡½æ•°
def analyze_question_intent(question: str) -> str:
    """åˆ†æé—®é¢˜æ„å›¾"""
    
    # ç®€å•çš„æ„å›¾åˆ†ç±»
    intents = {
        'product_info': ['äº§å“', 'åŠŸèƒ½', 'ä»·æ ¼', 'è§„æ ¼'],
        'technical_support': ['æ•…éšœ', 'é”™è¯¯', 'ä¸èƒ½', 'æ— æ³•', 'é—®é¢˜'],
        'account_management': ['è´¦æˆ·', 'ç™»å½•', 'å¯†ç ', 'æ³¨å†Œ'],
        'billing': ['è´¦å•', 'ä»˜è´¹', 'æ”¶è´¹', 'é€€æ¬¾'],
        'general_inquiry': ['å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'ä»€ä¹ˆæ—¶å€™']
    }
    
    question_lower = question.lower()
    
    for intent, keywords in intents.items():
        if any(keyword in question_lower for keyword in keywords):
            return intent
    
    return 'general_inquiry'

def generate_knowledge_based_answer(article: Dict, question: str) -> str:
    """åŸºäºçŸ¥è¯†åº“ç”Ÿæˆå›ç­”"""
    
    return f"""
æ ¹æ®æˆ‘ä»¬çš„çŸ¥è¯†åº“ï¼Œå…³äºæ‚¨çš„é—®é¢˜"{question}"ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š

{article['content']}

å¸Œæœ›è¿™ä¸ªå›ç­”å¯¹æ‚¨æœ‰å¸®åŠ©ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶è¯¢é—®ã€‚
"""

def generate_suggested_answer(articles: List[Dict], question: str) -> str:
    """ç”Ÿæˆå»ºè®®æ€§å›ç­”"""
    
    suggestions = []
    for article in articles[:2]:
        suggestions.append(f"â€¢ {article['title']}")
    
    return f"""
å…³äºæ‚¨çš„é—®é¢˜"{question}"ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€äº›å¯èƒ½ç›¸å…³çš„ä¿¡æ¯ï¼š

{chr(10).join(suggestions)}

å»ºè®®æ‚¨æŸ¥çœ‹è¿™äº›ç›¸å…³å†…å®¹ï¼Œæˆ–è€…æˆ‘å¯ä»¥ä¸ºæ‚¨è”ç³»äººå·¥å®¢æœè¿›è¡Œæ›´è¯¦ç»†çš„è§£ç­”ã€‚
"""

@mcp.resource("config://customer-service")
async def get_service_config() -> str:
    """è·å–å®¢æœé…ç½®"""
    
    config = {
        'response_templates': {
            'greeting': 'æ‚¨å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ',
            'closing': 'æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ä»¬ã€‚',
            'escalation': 'æˆ‘å°†ä¸ºæ‚¨è”ç³»äººå·¥å®¢æœï¼Œè¯·ç¨ç­‰ç‰‡åˆ»ã€‚',
            'not_found': 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®æ‚¨è”ç³»äººå·¥å®¢æœè·å¾—æ›´ä¸“ä¸šçš„å¸®åŠ©ã€‚'
        },
        'service_hours': {
            'ai_service': '24å°æ—¶å…¨å¤©å€™æœåŠ¡',
            'human_service': 'å·¥ä½œæ—¥ 9:00-18:00'
        },
        'escalation_rules': {
            'auto_escalate': ['æŠ•è¯‰', 'é€€æ¬¾', 'æ³•å¾‹'],
            'confidence_threshold': 0.3,
            'max_conversation_turns': 5
        }
    }
    
    return json.dumps(config, ensure_ascii=False, indent=2)
```

---

## 7. æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ

### 7.1 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### âš¡ å¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ

```python
import asyncio
import aiofiles
import aiohttp
from concurrent.futures import ThreadPoolExecutor
from functools import wraps
import time

class MCPPerformanceOptimizer:
    """MCP æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

# æ€§èƒ½ç›‘æ§è£…é¥°å™¨
def performance_monitor(func):
    """ç›‘æ§å·¥å…·æ‰§è¡Œæ€§èƒ½"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            await log_performance_metrics(
                tool_name=func.__name__,
                execution_time=execution_time,
                success=True,
                args_size=len(str(args) + str(kwargs))
            )
            
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            await log_performance_metrics(
                tool_name=func.__name__,
                execution_time=execution_time,
                success=False,
                error=str(e)
            )
            raise
    return wrapper

# ç¼“å­˜è£…é¥°å™¨
def cache_result(ttl: int = 300):
    """ç¼“å­˜å·¥å…·æ‰§è¡Œç»“æœ"""
    cache = {}
    
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # æ£€æŸ¥ç¼“å­˜
            if cache_key in cache:
                cached_result, timestamp = cache[cache_key]
                if time.time() - timestamp < ttl:
                    return cached_result
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = await func(*args, **kwargs)
            cache[cache_key] = (result, time.time())
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            current_time = time.time()
            cache = {k: v for k, v in cache.items() 
                    if current_time - v[1] < ttl}
            
            return result
        return wrapper
    return decorator

@mcp.tool()
@performance_monitor
@cache_result(ttl=600)
async def optimized_file_search(pattern: str, directory: str, max_results: int = 100) -> str:
    """ä¼˜åŒ–çš„æ–‡ä»¶æœç´¢å·¥å…·"""
    
    import asyncio
    from pathlib import Path
    
    async def search_directory(path: Path, pattern: str, results: list):
        """å¼‚æ­¥æœç´¢å•ä¸ªç›®å½•"""
        try:
            if len(results) >= max_results:
                return
            
            # ä½¿ç”¨å¼‚æ­¥æ–‡ä»¶æ“ä½œ
            for item in path.iterdir():
                if len(results) >= max_results:
                    break
                
                if item.name.match(pattern):
                    stat = await asyncio.get_event_loop().run_in_executor(
                        None, item.stat
                    )
                    results.append({
                        'path': str(item),
                        'size': stat.st_size,
                        'modified': stat.st_mtime
                    })
                
                if item.is_dir() and not item.name.startswith('.'):
                    await search_directory(item, pattern, results)
        
        except PermissionError:
            pass  # è·³è¿‡æ— æƒé™çš„ç›®å½•
    
    results = []
    base_path = Path(directory)
    
    # å¹¶å‘æœç´¢
    tasks = []
    for subdir in base_path.iterdir():
        if subdir.is_dir():
            task = search_directory(subdir, pattern, results)
            tasks.append(task)
    
    await asyncio.gather(*tasks, return_exceptions=True)
    
    return json.dumps({
        'pattern': pattern,
        'directory': directory,
        'results_count': len(results),
        'results': sorted(results, key=lambda x: x['modified'], reverse=True)
    }, ensure_ascii=False, indent=2)

@mcp.tool()
@performance_monitor
async def batch_api_requests(urls: List[str], timeout: int = 30) -> str:
    """æ‰¹é‡APIè¯·æ±‚ä¼˜åŒ–"""
    
    async with MCPPerformanceOptimizer() as optimizer:
        semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°
        
        async def fetch_url(url: str) -> Dict:
            async with semaphore:
                try:
                    async with optimizer.session.get(url, timeout=timeout) as response:
                        data = await response.text()
                        return {
                            'url': url,
                            'status': response.status,
                            'content_length': len(data),
                            'response_time': response.headers.get('X-Response-Time'),
                            'success': True
                        }
                except Exception as e:
                    return {
                        'url': url,
                        'error': str(e),
                        'success': False
                    }
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
        tasks = [fetch_url(url) for url in urls]
        results = await asyncio.gather(*tasks)
        
        summary = {
            'total_requests': len(urls),
            'successful_requests': sum(1 for r in results if r['success']),
            'failed_requests': sum(1 for r in results if not r['success']),
            'results': results
        }
        
        return json.dumps(summary, ensure_ascii=False, indent=2)
```

#### ğŸ—„ï¸ å†…å­˜ä¼˜åŒ–å’Œèµ„æºç®¡ç†

```python
import gc
import psutil
import sys
from typing import Generator
import weakref

class ResourceManager:
    """èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_resources = weakref.WeakSet()
        self.memory_threshold = 0.8  # 80% å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
    
    def monitor_memory(self):
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_percent = psutil.virtual_memory().percent / 100
        if memory_percent > self.memory_threshold:
            self.cleanup_resources()
            gc.collect()
    
    def cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        for resource in list(self.active_resources):
            if hasattr(resource, 'cleanup'):
                resource.cleanup()

resource_manager = ResourceManager()

@mcp.tool()
async def memory_efficient_file_processing(file_path: str, chunk_size: int = 1024*1024) -> str:
    """å†…å­˜é«˜æ•ˆçš„æ–‡ä»¶å¤„ç†"""
    
    async def process_file_chunks(file_path: str) -> Generator[str, None, None]:
        """åˆ†å—å¤„ç†å¤§æ–‡ä»¶"""
        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
            while True:
                chunk = await f.read(chunk_size)
                if not chunk:
                    break
                yield chunk
    
    try:
        processed_lines = 0
        total_chars = 0
        
        async for chunk in process_file_chunks(file_path):
            # å¤„ç†å—æ•°æ®
            lines_in_chunk = chunk.count('\n')
            processed_lines += lines_in_chunk
            total_chars += len(chunk)
            
            # å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨
            if processed_lines % 10000 == 0:
                resource_manager.monitor_memory()
        
        result = {
            'file_path': file_path,
            'processed_lines': processed_lines,
            'total_characters': total_chars,
            'memory_usage': f"{psutil.virtual_memory().percent:.1f}%"
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'å¤„ç†å¤±è´¥: {str(e)}'}, ensure_ascii=False)

@mcp.tool()
async def stream_large_dataset(data_source: str, batch_size: int = 1000) -> str:
    """æµå¼å¤„ç†å¤§æ•°æ®é›†"""
    
    async def data_generator(source: str, batch_size: int):
        """æ•°æ®ç”Ÿæˆå™¨"""
        if source.startswith('postgresql://'):
            # æ•°æ®åº“æµå¼è¯»å–
            import asyncpg
            
            conn = await asyncpg.connect(source)
            try:
                async with conn.transaction():
                    async for record in conn.cursor("SELECT * FROM large_table"):
                        yield record
            finally:
                await conn.close()
        
        elif source.endswith('.csv'):
            # CSV æµå¼è¯»å–
            import pandas as pd
            
            for chunk in pd.read_csv(source, chunksize=batch_size):
                for _, row in chunk.iterrows():
                    yield row.to_dict()
    
    try:
        processed_count = 0
        aggregated_stats = {
            'total_records': 0,
            'categories': {},
            'numerical_stats': {}
        }
        
        async for record in data_generator(data_source, batch_size):
            # å¤„ç†å•æ¡è®°å½•
            processed_count += 1
            aggregated_stats['total_records'] += 1
            
            # èšåˆç»Ÿè®¡ï¼ˆé¿å…å­˜å‚¨æ‰€æœ‰æ•°æ®ï¼‰
            for key, value in record.items():
                if isinstance(value, str):
                    if key not in aggregated_stats['categories']:
                        aggregated_stats['categories'][key] = {}
                    aggregated_stats['categories'][key][value] = \
                        aggregated_stats['categories'][key].get(value, 0) + 1
                
                elif isinstance(value, (int, float)):
                    if key not in aggregated_stats['numerical_stats']:
                        aggregated_stats['numerical_stats'][key] = {
                            'sum': 0, 'count': 0, 'min': float('inf'), 'max': float('-inf')
                        }
                    
                    stats = aggregated_stats['numerical_stats'][key]
                    stats['sum'] += value
                    stats['count'] += 1
                    stats['min'] = min(stats['min'], value)
                    stats['max'] = max(stats['max'], value)
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if processed_count % 10000 == 0:
                gc.collect()
                resource_manager.monitor_memory()
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        for key, stats in aggregated_stats['numerical_stats'].items():
            stats['average'] = stats['sum'] / stats['count'] if stats['count'] > 0 else 0
        
        return json.dumps(aggregated_stats, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({'error': f'æµå¼å¤„ç†å¤±è´¥: {str(e)}'}, ensure_ascii=False)
```

### 7.2 å®‰å…¨æœ€ä½³å®è·µ

#### ğŸ”’ è¾“å…¥éªŒè¯å’Œæƒé™æ§åˆ¶

```python
import os
import re
import hashlib
from pathlib import Path
from typing import Set, List
from pydantic import BaseModel, validator
import secrets

class SecurityConfig:
    """å®‰å…¨é…ç½®"""
    
    def __init__(self):
        self.allowed_paths: Set[Path] = {
            Path.home() / "Documents",
            Path.home() / "Downloads", 
            Path("/tmp")
        }
        self.forbidden_extensions = {'.exe', '.bat', '.cmd', '.scr', '.com'}
        self.max_file_size = 100 * 1024 * 1024  # 100MB
        self.rate_limits = {
            'requests_per_minute': 60,
            'requests_per_hour': 1000
        }

security_config = SecurityConfig()

class SecureFileOperation(BaseModel):
    """å®‰å…¨çš„æ–‡ä»¶æ“ä½œæ¨¡å‹"""
    
    file_path: str
    operation: str
    content: str = ""
    
    @validator('file_path')
    def validate_file_path(cls, v):
        """éªŒè¯æ–‡ä»¶è·¯å¾„å®‰å…¨æ€§"""
        path = Path(v).resolve()
        
        # é˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if '..' in v or v.startswith('/'):
            raise ValueError('ä¸å®‰å…¨çš„æ–‡ä»¶è·¯å¾„ï¼šåŒ…å«è·¯å¾„éå†')
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸çš„è·¯å¾„å†…
        is_allowed = any(
            path.is_relative_to(allowed_path) 
            for allowed_path in security_config.allowed_paths
        )
        
        if not is_allowed:
            raise ValueError(f'æ–‡ä»¶è·¯å¾„ä¸åœ¨å…è®¸èŒƒå›´å†…: {path}')
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if path.suffix.lower() in security_config.forbidden_extensions:
            raise ValueError(f'ä¸å…è®¸çš„æ–‡ä»¶ç±»å‹: {path.suffix}')
        
        return str(path)
    
    @validator('content')
    def validate_content_size(cls, v):
        """éªŒè¯å†…å®¹å¤§å°"""
        if len(v.encode('utf-8')) > security_config.max_file_size:
            raise ValueError('æ–‡ä»¶å†…å®¹è¿‡å¤§')
        return v
    
    @validator('operation')
    def validate_operation(cls, v):
        """éªŒè¯æ“ä½œç±»å‹"""
        allowed_operations = {'read', 'write', 'append', 'delete'}
        if v not in allowed_operations:
            raise ValueError(f'ä¸æ”¯æŒçš„æ“ä½œ: {v}')
        return v

class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self):
        self.requests = {}  # {client_id: [(timestamp, count), ...]}
    
    def is_rate_limited(self, client_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…è¿‡é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        
        if client_id not in self.requests:
            self.requests[client_id] = []
        
        # æ¸…ç†è¿‡æœŸè®°å½•
        self.requests[client_id] = [
            (timestamp, count) for timestamp, count in self.requests[client_id]
            if current_time - timestamp < 3600  # ä¿ç•™1å°æ—¶å†…çš„è®°å½•
        ]
        
        # æ£€æŸ¥æ¯åˆ†é’Ÿé™åˆ¶
        minute_requests = sum(
            count for timestamp, count in self.requests[client_id]
            if current_time - timestamp < 60
        )
        
        if minute_requests >= security_config.rate_limits['requests_per_minute']:
            return True
        
        # æ£€æŸ¥æ¯å°æ—¶é™åˆ¶
        hour_requests = sum(
            count for timestamp, count in self.requests[client_id]
        )
        
        if hour_requests >= security_config.rate_limits['requests_per_hour']:
            return True
        
        # è®°å½•æœ¬æ¬¡è¯·æ±‚
        self.requests[client_id].append((current_time, 1))
        return False

rate_limiter = RateLimiter()

def require_auth(func):
    """éœ€è¦è®¤è¯çš„è£…é¥°å™¨"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        # è¿™é‡Œå¯ä»¥æ·»åŠ è®¤è¯é€»è¾‘
        # ä¾‹å¦‚æ£€æŸ¥ API å¯†é’¥ã€JWT ä»¤ç‰Œç­‰
        
        # ç”Ÿæˆå®¢æˆ·ç«¯IDï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
        client_id = hashlib.md5(str(args).encode()).hexdigest()
        
        # æ£€æŸ¥é€Ÿç‡é™åˆ¶
        if rate_limiter.is_rate_limited(client_id):
            return json.dumps({
                'error': 'è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•'
            }, ensure_ascii=False)
        
        return await func(*args, **kwargs)
    return wrapper

@mcp.tool()
@require_auth
async def secure_file_operation(operation_data: SecureFileOperation) -> str:
    """å®‰å…¨çš„æ–‡ä»¶æ“ä½œ"""
    
    try:
        file_path = Path(operation_data.file_path)
        
        if operation_data.operation == 'read':
            # è¯»å–æ–‡ä»¶
            if not file_path.exists():
                return json.dumps({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}, ensure_ascii=False)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if file_path.stat().st_size > security_config.max_file_size:
                return json.dumps({'error': 'æ–‡ä»¶è¿‡å¤§'}, ensure_ascii=False)
            
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
            
            return json.dumps({
                'operation': 'read',
                'file_path': str(file_path),
                'content': content[:1000] + '...' if len(content) > 1000 else content,
                'size': len(content)
            }, ensure_ascii=False)
        
        elif operation_data.operation == 'write':
            # å†™å…¥æ–‡ä»¶
            # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(operation_data.content)
            
            return json.dumps({
                'operation': 'write',
                'file_path': str(file_path),
                'bytes_written': len(operation_data.content.encode('utf-8'))
            }, ensure_ascii=False)
        
        elif operation_data.operation == 'delete':
            # åˆ é™¤æ–‡ä»¶
            if file_path.exists():
                file_path.unlink()
                return json.dumps({
                    'operation': 'delete',
                    'file_path': str(file_path),
                    'status': 'deleted'
                }, ensure_ascii=False)
            else:
                return json.dumps({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}, ensure_ascii=False)
    
    except Exception as e:
        # è®°å½•å®‰å…¨äº‹ä»¶
        await log_security_event(
            event_type='file_operation_error',
            details={'error': str(e), 'operation': operation_data.dict()}
        )
        
        return json.dumps({'error': 'æ“ä½œå¤±è´¥'}, ensure_ascii=False)

@mcp.tool()
async def sanitize_user_input(input_text: str, input_type: str = "general") -> str:
    """æ¸…ç†ç”¨æˆ·è¾“å…¥"""
    
    sanitization_rules = {
        'general': {
            'max_length': 1000,
            'forbidden_patterns': [r'<script.*?>', r'javascript:', r'data:'],
            'allowed_chars': r'^[a-zA-Z0-9\s\u4e00-\u9fff.,!?()-]*$'
        },
        'filename': {
            'max_length': 255,
            'forbidden_patterns': [r'[<>:"/\\|?*]', r'^(CON|PRN|AUX|NUL|COM[1-9]|LPT[1-9])$'],
            'allowed_chars': r'^[a-zA-Z0-9._-]*$'
        },
        'sql': {
            'max_length': 500,
            'forbidden_patterns': [r'DROP\s+TABLE', r'DELETE\s+FROM', r'INSERT\s+INTO'],
            'allowed_chars': r'^[a-zA-Z0-9\s=(),._-]*$'
        }
    }
    
    rules = sanitization_rules.get(input_type, sanitization_rules['general'])
    
    # é•¿åº¦æ£€æŸ¥
    if len(input_text) > rules['max_length']:
        return json.dumps({
            'sanitized': False,
            'error': f'è¾“å…¥é•¿åº¦è¶…è¿‡é™åˆ¶ ({rules["max_length"]} å­—ç¬¦)'
        }, ensure_ascii=False)
    
    # æ¨¡å¼æ£€æŸ¥
    for pattern in rules['forbidden_patterns']:
        if re.search(pattern, input_text, re.IGNORECASE):
            return json.dumps({
                'sanitized': False,
                'error': f'è¾“å…¥åŒ…å«ç¦æ­¢çš„æ¨¡å¼: {pattern}'
            }, ensure_ascii=False)
    
    # å­—ç¬¦æ£€æŸ¥
    if not re.match(rules['allowed_chars'], input_text):
        # ç§»é™¤ä¸å…è®¸çš„å­—ç¬¦
        sanitized_text = re.sub(r'[^a-zA-Z0-9\s\u4e00-\u9fff.,!?()-]', '', input_text)
    else:
        sanitized_text = input_text
    
    return json.dumps({
        'original': input_text,
        'sanitized': sanitized_text,
        'is_safe': sanitized_text == input_text,
        'input_type': input_type
    }, ensure_ascii=False, indent=2)

async def log_security_event(event_type: str, details: dict):
    """è®°å½•å®‰å…¨äº‹ä»¶"""
    security_log = {
        'timestamp': datetime.now().isoformat(),
        'event_type': event_type,
        'details': details,
        'client_info': {
            'ip': 'unknown',  # åœ¨å®é™…åº”ç”¨ä¸­ä»è¯·æ±‚ä¸­è·å–
            'user_agent': 'unknown'
        }
    }
    
    # å†™å…¥å®‰å…¨æ—¥å¿—æ–‡ä»¶
    log_file = Path('security.log')
    async with aiofiles.open(log_file, 'a', encoding='utf-8') as f:
        await f.write(json.dumps(security_log, ensure_ascii=False) + '\n')
```

### 7.3 é”™è¯¯å¤„ç†å’Œç›‘æ§

#### ğŸš¨ å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶

```python
import traceback
import logging
from enum import Enum
from datetime import datetime
from typing import Optional, Any

class ErrorSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class MCPError(Exception):
    """è‡ªå®šä¹‰MCPå¼‚å¸¸"""
    
    def __init__(self, message: str, severity: ErrorSeverity = ErrorSeverity.MEDIUM, 
                 error_code: str = None, details: dict = None):
        self.message = message
        self.severity = severity
        self.error_code = error_code or "UNKNOWN_ERROR"
        self.details = details or {}
        self.timestamp = datetime.now()
        super().__init__(message)

class ErrorHandler:
    """é”™è¯¯å¤„ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger("mcp_server")
        self.error_stats = {
            'total_errors': 0,
            'by_severity': {severity.value: 0 for severity in ErrorSeverity},
            'by_type': {}
        }
    
    async def handle_error(self, error: Exception, context: dict = None) -> dict:
        """ç»Ÿä¸€é”™è¯¯å¤„ç†"""
        
        error_info = {
            'timestamp': datetime.now().isoformat(),
            'error_type': type(error).__name__,
            'message': str(error),
            'context': context or {}
        }
        
        if isinstance(error, MCPError):
            error_info.update({
                'severity': error.severity.value,
                'error_code': error.error_code,
                'details': error.details
            })
            severity = error.severity
        else:
            # æœªçŸ¥é”™è¯¯ï¼Œè®¾ä¸ºé«˜ä¸¥é‡çº§åˆ«
            severity = ErrorSeverity.HIGH
            error_info['severity'] = severity.value
            error_info['traceback'] = traceback.format_exc()
        
        # æ›´æ–°ç»Ÿè®¡
        self.error_stats['total_errors'] += 1
        self.error_stats['by_severity'][severity.value] += 1
        self.error_stats['by_type'][type(error).__name__] = \
            self.error_stats['by_type'].get(type(error).__name__, 0) + 1
        
        # è®°å½•æ—¥å¿—
        if severity == ErrorSeverity.CRITICAL:
            self.logger.critical(f"Critical error: {error_info}")
        elif severity == ErrorSeverity.HIGH:
            self.logger.error(f"High severity error: {error_info}")
        elif severity == ErrorSeverity.MEDIUM:
            self.logger.warning(f"Medium severity error: {error_info}")
        else:
            self.logger.info(f"Low severity error: {error_info}")
        
        # å…³é”®é”™è¯¯æ—¶å‘é€å‘Šè­¦
        if severity in [ErrorSeverity.CRITICAL, ErrorSeverity.HIGH]:
            await self.send_alert(error_info)
        
        return error_info
    
    async def send_alert(self, error_info: dict):
        """å‘é€é”™è¯¯å‘Šè­¦"""
        # è¿™é‡Œå¯ä»¥é›†æˆå„ç§å‘Šè­¦æ–¹å¼
        # ä¾‹å¦‚ï¼šé‚®ä»¶ã€Slackã€ä¼ä¸šå¾®ä¿¡ç­‰
        pass

error_handler = ErrorHandler()

def handle_exceptions(func):
    """å¼‚å¸¸å¤„ç†è£…é¥°å™¨"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        try:
            return await func(*args, **kwargs)
        
        except MCPError as e:
            # å·²çŸ¥çš„ MCP é”™è¯¯
            error_info = await error_handler.handle_error(e, {
                'function': func.__name__,
                'args': str(args)[:100],
                'kwargs': str(kwargs)[:100]
            })
            
            return json.dumps({
                'error': True,
                'error_code': e.error_code,
                'message': e.message,
                'severity': e.severity.value,
                'timestamp': error_info['timestamp']
            }, ensure_ascii=False)
        
        except Exception as e:
            # æœªé¢„æœŸçš„é”™è¯¯
            error_info = await error_handler.handle_error(e, {
                'function': func.__name__,
                'args': str(args)[:100],
                'kwargs': str(kwargs)[:100]
            })
            
            return json.dumps({
                'error': True,
                'error_code': 'UNEXPECTED_ERROR',
                'message': 'æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•',
                'severity': 'high',
                'timestamp': error_info['timestamp']
            }, ensure_ascii=False)
    
    return wrapper

@mcp.tool()
@handle_exceptions
async def robust_data_processing(data_source: str, processing_type: str) -> str:
    """å¥å£®çš„æ•°æ®å¤„ç†å·¥å…·"""
    
    # è¾“å…¥éªŒè¯
    if not data_source:
        raise MCPError(
            "æ•°æ®æºä¸èƒ½ä¸ºç©º",
            ErrorSeverity.MEDIUM,
            "INVALID_INPUT"
        )
    
    valid_types = ['csv', 'json', 'xml', 'database']
    if processing_type not in valid_types:
        raise MCPError(
            f"ä¸æ”¯æŒçš„å¤„ç†ç±»å‹: {processing_type}",
            ErrorSeverity.MEDIUM,
            "UNSUPPORTED_TYPE",
            {"valid_types": valid_types}
        )
    
    try:
        # æ£€æŸ¥æ•°æ®æºå¯ç”¨æ€§
        if not await check_data_source_availability(data_source):
            raise MCPError(
                "æ•°æ®æºä¸å¯è®¿é—®",
                ErrorSeverity.HIGH,
                "DATA_SOURCE_UNAVAILABLE",
                {"data_source": data_source}
            )
        
        # æ‰§è¡Œæ•°æ®å¤„ç†
        result = await process_data(data_source, processing_type)
        
        return json.dumps({
            'success': True,
            'result': result,
            'processed_at': datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)
        
    except FileNotFoundError:
        raise MCPError(
            f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {data_source}",
            ErrorSeverity.MEDIUM,
            "FILE_NOT_FOUND"
        )
    
    except PermissionError:
        raise MCPError(
            f"æ²¡æœ‰æƒé™è®¿é—®: {data_source}",
            ErrorSeverity.HIGH,
            "PERMISSION_DENIED"
        )
    
    except MemoryError:
        raise MCPError(
            "æ•°æ®é›†è¿‡å¤§ï¼Œå†…å­˜ä¸è¶³",
            ErrorSeverity.CRITICAL,
            "OUT_OF_MEMORY"
        )

# é‡è¯•æœºåˆ¶
class RetryConfig:
    def __init__(self, max_attempts: int = 3, base_delay: float = 1.0, 
                 exponential_backoff: bool = True):
        self.max_attempts = max_attempts
        self.base_delay = base_delay
        self.exponential_backoff = exponential_backoff

def retry_on_failure(config: RetryConfig):
    """å¤±è´¥é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(config.max_attempts):
                try:
                    return await func(*args, **kwargs)
                
                except Exception as e:
                    last_exception = e
                    
                    # ä¸é‡è¯•çš„é”™è¯¯ç±»å‹
                    if isinstance(e, MCPError) and e.severity == ErrorSeverity.CRITICAL:
                        raise
                    
                    if attempt < config.max_attempts - 1:
                        # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                        if config.exponential_backoff:
                            delay = config.base_delay * (2 ** attempt)
                        else:
                            delay = config.base_delay
                        
                        await asyncio.sleep(delay)
                        
                        # è®°å½•é‡è¯•
                        logging.warning(f"Retrying {func.__name__} (attempt {attempt + 2}/{config.max_attempts}) after {delay}s delay")
            
            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
            raise last_exception
        
        return wrapper
    return decorator

@mcp.tool()
@retry_on_failure(RetryConfig(max_attempts=3, base_delay=2.0))
@handle_exceptions
async def reliable_api_call(url: str, method: str = "GET", headers: dict = None) -> str:
    """å¯é çš„APIè°ƒç”¨å·¥å…·"""
    
    import aiohttp
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.request(method, url, headers=headers) as response:
                if response.status >= 400:
                    raise MCPError(
                        f"APIè°ƒç”¨å¤±è´¥: HTTP {response.status}",
                        ErrorSeverity.HIGH,
                        "API_ERROR",
                        {"status_code": response.status, "url": url}
                    )
                
                data = await response.text()
                
                return json.dumps({
                    'url': url,
                    'method': method,
                    'status_code': response.status,
                    'content_length': len(data),
                    'data': data[:1000] + '...' if len(data) > 1000 else data
                }, ensure_ascii=False, indent=2)
        
        except aiohttp.ClientTimeout:
            raise MCPError(
                "APIè°ƒç”¨è¶…æ—¶",
                ErrorSeverity.MEDIUM,
                "TIMEOUT_ERROR",
                {"url": url}
            )
        
        except aiohttp.ClientConnectionError:
            raise MCPError(
                "æ— æ³•è¿æ¥åˆ°APIæœåŠ¡",
                ErrorSeverity.HIGH,
                "CONNECTION_ERROR",
                {"url": url}
            )

@mcp.resource("monitoring://error-stats")
async def get_error_statistics() -> str:
    """è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯"""
    
    return json.dumps({
        'error_statistics': error_handler.error_stats,
        'uptime': get_server_uptime(),
        'last_updated': datetime.now().isoformat(),
        'health_status': get_health_status()
    }, ensure_ascii=False, indent=2)

def get_server_uptime() -> str:
    """è·å–æœåŠ¡å™¨è¿è¡Œæ—¶é—´"""
    # ç®€åŒ–å®ç°
    return "è¿è¡Œæ­£å¸¸"

def get_health_status() -> str:
    """è·å–å¥åº·çŠ¶æ€"""
    total_errors = error_handler.error_stats['total_errors']
    critical_errors = error_handler.error_stats['by_severity']['critical']
    
    if critical_errors > 0:
        return "critical"
    elif total_errors > 100:
        return "warning"
    else:
        return "healthy"
```

---

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "\u4e0b\u8f7d\u77e5\u4e4e\u6587\u7ae0\u4e2d\u7684\u914d\u56fe\u5e76\u6dfb\u52a0\u5230\u6587\u6863", "status": "completed", "activeForm": "\u4e0b\u8f7d\u77e5\u4e4e\u6587\u7ae0\u4e2d\u7684\u914d\u56fe\u5e76\u6dfb\u52a0\u5230\u6587\u6863"}, {"content": "\u9605\u8bfb\u77e5\u4e4e\u6587\u7ae0\u4e2d\u5f15\u7528\u7684\u5176\u4ed6\u6587\u7ae0", "status": "completed", "activeForm": "\u9605\u8bfb\u77e5\u4e4e\u6587\u7ae0\u4e2d\u5f15\u7528\u7684\u5176\u4ed6\u6587\u7ae0"}, {"content": "\u91cd\u65b0\u7ec4\u7ec7\u6587\u6863\u76ee\u5f55\u7ed3\u6784", "status": "completed", "activeForm": "\u91cd\u65b0\u7ec4\u7ec7\u6587\u6863\u76ee\u5f55\u7ed3\u6784"}, {"content": "\u5b8c\u5584\u6587\u6863\u5185\u5bb9", "status": "in_progress", "activeForm": "\u5b8c\u5584\u6587\u6863\u5185\u5bb9"}, {"content": "\u63d0\u4ea4\u5e76\u63a8\u9001\u66f4\u65b0", "status": "pending", "activeForm": "\u63d0\u4ea4\u5e76\u63a8\u9001\u66f4\u65b0"}]